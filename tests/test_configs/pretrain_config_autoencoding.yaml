data_arguments:
    dataset_names: ["wikitext_v1"]
    interleave_datasets: true
    interleave_probabilities: [0.5, 0.5]
    num_proc: 6
    mlm_probability: 0.15

model_arguments:
    model_type: "autoencoding"
    model_name: "Maltehb/danish-bert-botxo"

training_arguments:
    learning_rate: 0.0005
    do_train: true
    do_eval: true
    push_to_hub: false
    fp16: true
    max_steps: 15
    evaluation_strategy: "steps"
    output_dir: "models123"
