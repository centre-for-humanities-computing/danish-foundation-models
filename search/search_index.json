{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Empowering the Danish Language in the Digital Age","text":"<p>Welcome to the Danish Foundation Models (DFM) project, a pioneering initiative in the field of machine learning and natural language processing (NLP) dedicated to the Danish language. Our mission is to develop, maintain, and provide open access to high-quality foundation models tailored for Danish, promoting innovation and inclusivity in language technologies.</p> <p>Read the paper</p> <p>You can read more about the argument for Danish Language models in our publication.</p>"},{"location":"#why-danish-foundation-models","title":"Why Danish Foundation Models?","text":""},{"location":"#bridging-the-digital-language-divide","title":"Bridging the Digital Language Divide","text":"<ul> <li>Global Gap: The rise of large language models has transformed research and technology, but smaller languages like Danish risk falling behind both in development, evaluation and application.</li> <li>Local Focus: We combat this by focusing on the Danish language, ensuring that it is well-represented in the digital landscape.</li> <li>Broad Collaboration: Our project unites public and private institutions, ensuring high data quality and practical applicability of our models.</li> </ul>"},{"location":"#our-objectives","title":"Our Objectives","text":"<ol> <li>To develop and maintain state-of-the-art language models for Danish for applications within both text and speech.</li> <li>To extensively validate foundation models for Danish in a representative set of tasks.</li> <li>To maintain a high standard of documentation of models such as model cards [Mitchell et al., 2019] and datasheets [Gebru et al., 2021].</li> <li>To open-source not only the models but also all components required for reproducibility such as pre-processing, training, and validation code.</li> </ol>"},{"location":"#open-source","title":"Open-source","text":""},{"location":"#open-source-development-with-privacy-focused-data-handling","title":"Open-source Development with Privacy-Focused Data Handling","text":"<p>In our commitment to advancing open-source development, we strongly emphasise the ethical handling of data, particularly when it involves personally sensitive information or material under copyright. This ensures that we share as much as possible while protecting privacy.</p> <p>To achieve this, our project is structured to differentiate between data that can be shared openly and that which cannot.  This demarcation is documented through detailed datasheets and training logs, thereby ensuring transparency in our processes.</p> <p>Additionally, we prioritise the security of the data during its processing and training phases. All data is stored on UCloud, a platform that upholds the recognised highest standards in information security management. This commitment to data security is exemplified by UCloud's adherence to ISO27001, a globally recognised standard, ensuring that our data handling practices meet rigorous international criteria. For more information on our security measures, please visit UCloud's security documentation.</p> <p></p>"},{"location":"#contributions","title":"Contributions","text":"<p>Besides our models DFM has led to a series of positive open-source contributions, the following table include some of these contributions:</p> Project Contribution Packages NLPDedup A deduplication library derived from DFM's deduplication code Code contributions TextDescriptives Added heuristic quality measure for texts dolma Bugfixes and addition of taggers for filtering Benchmarks ScandEval Co-contributors have significant contributions to developing NLU and NLG benchmarks for Scandinavian and Germanic languages Scandinavian Embedding Benchmark The benchmark for evaluating Scandinavian embedding has been created as a part of DFM Datasets m_arc, m_mmlu, m_hellaswag, m_truthfulqa Translated versions of English datasets intended for model evaluation for these domains"},{"location":"#improving-the-danish-language-technology-landscape","title":"Improving the Danish Language Technology Landscape","text":"<p>The Danish Foundations models is a collaboration across Danish universities and research organizations. The project engages with data science communities and initiatives (Danish Data Science Community), to promote the development of Danish language tools. We continually gather information about how to improve the Danish language technologies and how to best support the community. If you want to highlight missing pieces in Danish NLP we invite you to open a thread on the forum stating the problems and potential solutions.</p>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"#the-core-team","title":"The Core Team","text":"<p>Those with data access, who contribute to the project, including data management, model development, project management, and more.</p> <p>From the Center for Humanities Computing at Aarhus University:</p> <ul> <li>Kenneth Enevoldsen (kenneth.enevoldsen@cas.au.dk)</li> <li>Marton Kardos (martonkardos@cas.au.dk)</li> <li>Jan Kostkan (jan.kostkan@cas.au.dk)</li> <li>Peter Vahlstrup (imvpbv@cc.au.dk)</li> <li>Per M\u00f8ldrup-Dalum (per@cas.au.dk)</li> <li>Kristoffer Laigaard Nielbo (kln@cas.au.dk)</li> </ul> <p>From the Alexandra Institute:</p> <ul> <li>Rasmus Larsen (rasmus.larsen@alexandra.dk)</li> <li>Dan Saattrup Nielsen (dan.nielsen@alexandra.dk)</li> <li>Andreas Nugaard Holm (andreas.holm@alexandra.dk)</li> <li>Kristian N\u00f8rgaaard Jensen (kristian.n.jensen@alexandra.dk)</li> <li>Torben Blach (torben.blach@alexandra.dk)</li> <li>Jens Kaas Benner (jens.benner@alexandra.dk)</li> </ul> <p>From the Center for Machine Learning at the University of Southern Denmark:</p> <ul> <li>Peter Schneider-Kamp (petersk@imada.sdu.dk)</li> <li>Lukas Galke (galke@imada.sdu.dk)</li> <li>Andrea Blasi N\u00fa\u00f1ez (abln@mmmi.sdu.dk)</li> <li>Gianluca Barmina (gbarmina@imada.sdu.dk)</li> <li>Jacob Nielsen (jacn@imada.sdu.dk)</li> <li>Mogens Henrik From (from@imada.sdu.dk)</li> <li>Stine Lyngs\u00f8 Beltoft (stinelb@imada.sdu.dk)</li> </ul> <p>From the Department of Computer Science at the University of Copenhagen:</p> <ul> <li>Desmond Elliott (de@di.ku.dk)</li> </ul> <p>From Center for Sprogteknologi at the University of Copenhagen:</p> <ul> <li>Bolette Sandford Pedersen (bspedersen@hum.ku.dk</li> <li>Ali Basirat (alib@hum.ku.dk)</li> </ul> Project Alumnis   Lasse Hansen, Martin Bernstorff, Tao Tang"},{"location":"#core-contributors","title":"Core Contributors","text":"<p>Those without data access, but who have contributed substantially to the project including code contributions, model development, and experiment planning.</p> <p>From Alvenir:</p> <ul> <li>Martin Carsten Nielsen (martin@alvenir.ai)</li> <li>S\u00f8ren Vejlgaard Holm (swh@alvenir.ai)</li> </ul>"},{"location":"#join-us","title":"Join Us","text":"<p>We invite collaboration and contributions from industry professionals, researchers, and the open-source community. Together, we can advance the field of Danish NLP and create a more inclusive digital future. You can reach out to us using the following channels:</p>  - DDSC Slack Join the discussion in the \"danish-foundation-models-text\"-channel  -  GitHub Discussion Ask questions or start a discussion  - GitHub Issues Noticed a bug in the code? Please create an issue  - Using the model? If you use the model, let us know it makes it easier for us to apply for funding and justify the devopment of the project. <p>Contact us </p>"},{"location":"dcc/","title":"DCC <sub>v1</sub>","text":"<p>The DCC is a composite corpus consisting of the following subcorpora. For more information about the specific subcorpora, feel free to check out the individual datasheets.</p> Name Description Size Open Access Novel Corpus Text DAGW Danish Gigaword 1B tokens \u2713 \u2717 reddit-da Danish Reddit &lt;.1B tokens \u2713 \u2717 HopeTwitter Danish Tweets 0.48B tokens \u2717 \u2713 DaNews Danish newspapers 0.5B tokens \u2717 \u2713 Netarkivet Text Danish internet &gt;100B tokens \u2717 \u2713 Speech DaRadio Danish talk radio 140,000 hours \u2717 \u2713 DaTV Danish subtitled TV 900 hours \u2717 \u2713"},{"location":"dcc/#collaborators-and-data-owners","title":"Collaborators and Data Owners","text":"<p>Data are provided in agreement with the data owners and data collaborators. The data is generally accecible by the research collaborators, though  each data agreements has their own access restrictions and might not cover all research collaborators. Access restriction are specified on the server hosting the data in accordance with the data agreements.</p> <ul> <li>Data Owners</li> <li>Aviser / dagblade</li> <li>Danmarks Statistik</li> <li>NetArkivet</li> <li>Data Collaborators</li> <li>Det Kongelige bibliotek</li> <li>Infomedia</li> <li>Research Collaborators</li> <li>Center for humanities Computing, Aarhus Universitet</li> <li>Alexandra Institutet</li> <li>Peter Schneider-Kamp, Syddansk Universitet</li> </ul>"},{"location":"intercoder_reliability/","title":"Results from corpus tagging","text":"<p>Each user tagged 100 documents unless otherwise specified. Documents were split by newlines into text-blocks, block was rated. Text-blocks longer than 1000 characters were split into multiple blocks of 1000 characters or less.</p> <p>This tagging scheme is similar to (Kreutzer et al., 2022).</p> <p>Each block was put into one of the following categories: Each user tagged 100 documents (unless otherwise specified). Each document were tagged</p> <ul> <li><code>wrong_language</code>: Not Danish</li> <li><code>skipped</code>: Unsure of category</li> <li><code>correct_language</code>: Danish text where at least 80% of the text is reasonable.</li> <li><code>not_language</code>: Text where less than 80% of the text is reasonable. Takes priority over <code>wrong_language</code>.</li> </ul> <p>Additionally, each block was tagged for pornography (yes/no) and offensiveness (yes/no).</p>"},{"location":"intercoder_reliability/#text-proportions","title":"Text proportions","text":"<p>Kenneth (Session: test)</p> <ul> <li>Date: 2022-09-05</li> <li>Sentences tagged: 102</li> <li>Documents tagged: na</li> </ul> <p>Proportions:</p> <ul> <li>69.16% of characters is <code>correct_language</code></li> <li>25.66% of characters is <code>not_language</code></li> <li>2.74% of characters is <code>skipped</code></li> <li>2.45% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 292</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.03% of characters is <code>correct_language</code></li> <li>29.19% of characters is <code>not_language</code></li> <li>2.10% of characters is <code>skipped</code></li> <li>0.68% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>1.38% of characters is offensive</li> </ul> <p>Lasse (Session: 1)</p> <ul> <li>Date: 2022-09-07</li> <li>Sentences tagged: 336</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.02% of characters is <code>correct_language</code></li> <li>30.97% of characters is <code>not_language</code></li> <li>1.01% of characters is <code>wrong_language</code></li> <li>0.26% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"intercoder_reliability/#intercoder-reliability","title":"Intercoder Reliability","text":"<p>Kenneth (Session: test) vs Kenneth - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8242 (Overlap in sentences: 98)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.9075 (Overlap in sentences: 98)</p> </li> </ul> <p>Kenneth (Session: test) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8140 (Overlap in sentences: 95)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.8389 (Overlap in sentences: 95)</p> </li> </ul> <p>Kenneth (Session: 1) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.6767 (Overlap in sentences: 245)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.7259 (Overlap in sentences: 245)</p> </li> </ul> <p>Comparison with mC4</p> <p>Note: mC4 did have a high degree of repititious texts. Similarly it did when texts blocks where not language they were often something like:</p> <pre><code>2lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22nowiki%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;]&amp;#x200b;&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&amp;#x200b;&lt;/span&gt;, at en lurifaks som Jimmy page, bruger MIT navn til opfindelsen! SV&lt;span data-rte-instance=\"\"1524-12953202845f3523698f3f1\"\" data-rte-meta=\"\"%7B%22type%22%3A%22ext%22%2C%22wikitext%22%3A%22%3Cref%3ESVIN%3C%5C%2Fref%3E%22%2C%22lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22ref%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;&lt;sup data-rte-washtml=\"\"1\"\" id=\"\"cite_ref-2\"\" class=\"\"reference\"\" data-rte-attribs=\"\"\n</code></pre> <p>While non-language texts in NAT was often menu bars, contact information, or navigation.</p> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 325</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>62.47% of characters is <code>correct_language</code></li> <li>34.88% of characters is <code>not_language</code></li> <li>1.27% of characters is <code>skipped</code></li> <li>1.38% of characters is <code>wrong_language</code></li> <li>3.25% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"models/","title":"Models","text":"<p>This section gives an overview of the models available through the DFM project. The models are available through the Huggingface model hub. To avoid duplicating information surrounding the models and the information regarding the models are available at the models model sheet.</p>"},{"location":"models/#text-models","title":"Text Models","text":"Model Model type Size (parameters) munin-7b-alpha Decoder 7.24B encoder-large-v1 Encoder large (355M) encoder-medium-v1 Encoder medium (110M) encoder-small-v1 Encoder small (22M)"},{"location":"models/#speech-models","title":"Speech Models","text":"Model Model type xls-r-300m-danish Pretrained wav2vec2.0 model xls-r-300m-danish-nst-cv9 Automatic speech recognition chcaa/xls-r-300m-nst-cv9-da Automatic speech recognition"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/","title":"The Imperative of Danish Foundation Models: Bridging the Linguistic AI Divide","text":"<p>In recent years, the field of machine learning has experienced a transformative shift, primarily driven by the advent of foundation models. These models, pre-trained on vast amounts of data, can be finetuned for various downstream tasks, making them invaluable across multiple domains. However, the dominance of the English language in the development of these models poses significant challenges for smaller language communities. The Danish Foundation Models project emerges as a crucial initiative to ensure that the Danish language does not lag behind in this AI revolution.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/#the-case-for-danish-foundation-models","title":"The Case for Danish Foundation Models","text":"<p>The global landscape of foundation models is heavily skewed towards English, with few models catering to other languages. Although multilingual models exist, they often fail to capture the unique linguistic and cultural nuances of smaller languages like Danish. This discrepancy is particularly evident in practical applications where cultural context matters, such as healthcare services or public administration. The Danish Foundation Models project aims to fill this gap by developing high-quality, open-source foundation models specifically for the Danish language.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/#challenges-in-developing-danish-language-models","title":"Challenges in Developing Danish Language Models","text":"<ol> <li> <p>Computational Resources: Danish models have historically been trained with limited computational resources compared to their English counterparts. This disparity in resources leads to less effective models.</p> </li> <li> <p>Data Quality and Quantity: The datasets available for training Danish models are significantly smaller and less diverse. High-quality benchmarks and datasets, crucial for training robust models, are often lacking.</p> </li> <li> <p>Model Documentation: Proper documentation, including model cards and datasheets, is essential for ensuring AI models' ethical and effective use. Danish models frequently suffer from inadequate documentation, impeding their adoption in critical sectors.</p> </li> </ol>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/#the-danish-foundation-models-project","title":"The Danish Foundation Models Project","text":"<p>To address these challenges, the Danish Foundation Models (DFM) project has outlined four primary objectives:</p> <ol> <li> <p>Developing State-of-the-Art Models: Creating and maintaining advanced language models for Danish text and speech applications.</p> </li> <li> <p>Extensive Validation: Rigorous testing of these models across a representative set of tasks to ensure their efficacy and reliability.</p> </li> <li> <p>High-Quality Documentation: Maintaining comprehensive documentation for all models, promoting transparency and trust.</p> </li> <li> <p>Open-Source Collaboration: Ensuring that all models and their training processes are openly available to the community, fostering reproducibility and further innovation.</p> </li> </ol>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/#future-directions","title":"Future Directions","text":"<p>The DFM project plans to develop open-source language models for NLP, NLU, and ASR systems in Danish. Upcoming benchmarks will include data from diverse domains, such as healthcare and legal, ensuring comprehensive evaluation criteria for future models.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide/#conclusion","title":"Conclusion","text":"<p>The Danish Foundation Models project exemplifies a concerted effort to bridge the linguistic AI divide. By focusing on high-quality, well-documented, and openly accessible models, the DFM initiative not only safeguards the Danish language's digital presence but also sets a precedent for other smaller language communities. As we move forward, the collaboration between academia, industry, and the open-source community will be pivotal in sustaining and advancing this crucial work.</p>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/","title":"Tutorial: Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</code></pre> <pre><code># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#configure-the-model","title":"Configure the Model","text":"<pre><code>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#load-the-model","title":"Load the Model","text":"<pre><code>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#load-and-prepare-data","title":"Load and Prepare Data","text":"<p>Load the dataset from Hugging Face Hub:</p> <pre><code>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</code></pre> <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> <pre><code>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</code></pre> <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> <pre><code>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#finetune","title":"Finetune!","text":"<pre><code>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</code></pre> <pre><code># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</code></pre> <pre><code># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</code></pre> <pre><code># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#try-it-out","title":"Try it Out","text":"<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</code></pre> <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks","text":""},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM","text":"<p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> <pre><code># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> <pre><code># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models/#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion","text":"<p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> <pre><code># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/","title":"Tutorial: Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook.  It has descriptions and examples for the different merge methods it supports.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</code></pre> <pre><code># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#configure-the-merge","title":"Configure the Merge","text":"<p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> <pre><code>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</code></pre> <pre><code>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#merge","title":"Merge!","text":"<pre><code>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#try-it-out","title":"Try it Out","text":"<p>Time to try out the new merged model. Let's start by loading it from disk.</p> <pre><code>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</code></pre> <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</code></pre> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models/#clean-up","title":"Clean Up","text":"<p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> <pre><code># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</code></pre>"},{"location":"blog/2024/01/11/releasing-munin-7b-alpha---a-danish-llm/","title":"Releasing Munin 7B Alpha - A Danish LLM","text":"<p>We are excited to announce the release of the first model from the Danish Foundation Models project, nicknamed Munin 7B Alpha. This model represents the beginning of our research into Danish Large Language Models (LLMs), employing continual pre-training based on the already pre-trained Mistral-7b-v0.1 model. It has been pre-trained on the Danish Gigaword dataset, which has been instrumental in training various Danish BERT-style models.</p> <p>The model has been trained for one epoch over the dataset and ends up with a loss 1.27 on the Danish Gigaword. See more the model training logs here.</p> <p>This release underscores our commitment to transparency about our work and the challenges we are facing. We want to clearly note that we expect the model to perform suboptimally for many, if not most, applications. Our evaluations on the limited generative Danish tasks available to us have indicated that our current training approach may negatively impact performance on these downstream tasks, even compared to the upstream Mistral model.</p> Model Name Overall Score Danish Score Norwegian Score Swedish Score gpt-3.5-turbo-0613 58.52 \u00b1 2.42 56.72 \u00b1 2.44 57.31 \u00b1 2.37 61.54 \u00b1 2.46 mistralai/Mistral-7B-v0.1 40.30 \u00b1 2.15 39.60 \u00b1 1.94 35.98 \u00b1 2.54 45.31 \u00b1 1.96 danish-foundation-models/munin-7b-alpha 37.50 \u00b1 2.49 39.56 \u00b1 2.70 30.82 \u00b1 2.69 42.13 \u00b1 2.07 AI-Sweden-Models/gpt-sw3-6.7b-v2 26.67 \u00b1 2.30 23.65 \u00b1 2.02 24.28 \u00b1 2.74 32.08 \u00b1 2.13 mhenrichsen/danskgpt-tiny 16.87 \u00b1 3.05 16.66 \u00b1 2.18 15.16 \u00b1 2.64 18.80 \u00b1 4.35 <p>See the full ScandEval leaderboard for an up-to-date comparison. Despite these challenges, we hope that our open approach encourages the community to collaborate with us in building the best possible Danish LLM. While the current version of the model may not yet be a practical tool for Danish NLP, we believe that sharing our findings is valuable. A critical need has been identified: access to a significantly larger corpus of Danish text data, and a legal framework that reliably allows for training and releasing open models, including for commercial use.</p> <p>At Danish Foundation Models, we are actively pursuing legal access to extensive Danish text data, and are exploring every option for releasing models under the most open license possible. We have already secured agreements that provide us access to several large Danish datasets, and we plan to include these into our training process in the near future.</p> <p>In summary, Munin 7B Alpha is a small step forward. It signifies our commitment to advancing Danish NLP and acknowledges the extensive work ahead. By sharing this model, we aim to foster collaborative efforts within the community. The model is now available for download and experimentation, and we look forward to your insights and discussions on how we can progress.</p> <p>The development of this model, and the Danish Foundation Models project in general, is generously supported by the following:</p> <ul> <li>Danish e-infrastructure Consortium</li> <li>Acquisition and Logistics Organisation at the Danish Ministry of Defence</li> <li>Danish Ministry of Higher Education and Science under the Digital Security, Trust   and Data Ethics performance contract</li> </ul>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/","title":"Datah\u00e5ndtering","text":"<p>For at kunne tr\u00e6ne sprogmodeller (Large Language Models, LLM) skal der store m\u00e6ngder af data til. Fra vi modtager r\u00e5data til at de kan bruges til at tr\u00e6ne sprogmodeller p\u00e5, gennemg\u00e5r de en transformationsprocess.</p> <p>F\u00f8lgende er en overordnet beskrivelse af denne processen. Vi udvikler og forbedre l\u00f8bende processen, for at sikre at vi bruger state-pf-the-art metoder og praksis.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#sikker-handtering","title":"Sikker h\u00e5ndtering","text":"<p>I Danish Foundation Model bruger vi Danish e-infrastructure Consortium DeiC og UCloud til data h\u00e5ndtering. UCloud platformen er ISO27001 godkendt. Det er globalt anerkendt standard, der sikrer at vores datah\u00e5ndteringspraksis opfylder strenge internationale kriterier. For mere information om vores sikkerhedsforanstaltninger, se UClouds sikkerhedsdokumentation.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#dataklargrelse","title":"Dataklarg\u00f8relse","text":"<p>Alt dataklarg\u00f8relse foreg\u00e5r p\u00e5 UCloud. Figuren viser den proces alt data skal igennem f\u00f8r det bruges til tr\u00e6ning af sprogmodellen. Det r\u00e5 data beholdes i sin oprindelige form p\u00e5 UCloud. Derefter annoteres de r\u00e5 data med metadata.</p> <p>Dette datas\u00e6t overf\u00f8res til en GPU-accelereret supercomputer igennem en sikker forbindelse, hvorefter selve tr\u00e6ningen af modellen begyndes. Under tr\u00e6ningen gemmes flere checkpoints med modelv\u00e6gte. De gemte checkpoints med modelv\u00e6gte publiceres sammen med modelkode og anvendes til at k\u00f8re modellen. De tre processer er beskrevet i detalje nedenfor.</p> <p></p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#metadata-og-formatering","title":"Metadata og formatering","text":"<p>Det r\u00e5 data annoteres med to typer af metadata. Den f\u00f8rste type er et datablad (i Markdown, som i HuggingFace dataset cards der opsummerer hele datas\u00e6ttet og beskriver bl.a. proveniens og hvilken licens der er p\u00e5lagt det givne datas\u00e6t. Et udsnit af et databladseksempel er vist nedenfor. Den f\u00f8rste del af databladet er annoteret i et maskinvenligt format, som g\u00f8r det muligt automatisk at udv\u00e6lge datas\u00e6ttet blandt en st\u00f8rre samling. Resten af databladet giver en dybere beskrivelse af datas\u00e6ttet i fritekst.</p> <pre><code>---\npretty_name:  Scrape from Hovedstaden\nlanguage:\n  - da\nlicense: cc0-1.0\nlicense_name: Creative Commons Zero v1.0 Universal\nsize_categories:\n  - 10K&lt;n&lt;100K\ntask_categories:\n  - text-generation\n  - fill-mask\ntask_ids:\n  - language-modeling\n---\n# Dataset Card for scape_hovedstaden\n## Dataset Description\n- **Number of records:** 24752\n- **Languages:** Danish\n</code></pre> <p>Den anden type af metadata er per-dokument metadata, der beskriver hvilket datas\u00e6t dokumentet h\u00f8rer til, hvor det stammer fra, hvorn\u00e5r det er tilf\u00f8jet, samt andre metadata som f.eks. fra hvilken URL dokumentet kommer fra. Per-dokument metadata gemmes sammen med dokumentet i et standardiseret jsonl format. Et eksempel p\u00e5 et enkelt dokument inklusiv metadata fra datas\u00e6ttet \"Scrape from Hovedstaden\" er vist nedenfor. Disse metadata f\u00f8lger dokumentet igennem hele processeringen, s\u00e5 det er muligt at spore dokumenterne tilbage til kilden fra det endelige tr\u00e6ningskorpus. For hvert r\u00e5 datas\u00e6t vedligeholdes et script der kan bruges til konvertering af de r\u00e5 data til det standardiserede format.</p> <pre><code>{\n    'id': 'doc_hovedstaden_Rigshospitalet_Bed\u042bvelse og Intensiv Behandling (NEU)_Transkraniel Doppler - NIA 6021',\n    'text': 'Transkraniel Doppler - NIA 6021\\n\\nM\u00e5lgrupper og anv...',\n    'source': 'scrape_hovedstaden',\n    'added': '2024-05-23',\n    'created': '2023-11-16, 2024-04-04',\n    'metadata': {\n        'subject': 'health',\n        'language': 'danish',\n        'organization': 'The Danish Agency for Digitalisation',\n        'source-pretty': 'University of Southern Denmark (SDU) &amp; Capital Region',\n        'URL': 'https://sprogteknologi.dk/dataset/1076892a-14ee-4f14-a9db-32efb03c40c9'\n    }\n}\n</code></pre> <p>Flere detaljer om formatet er beskrevet her.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#filtrering","title":"Filtrering","text":"<p>Det standardiserede format muligg\u00f8r en ensartet processering af dokumenterne. De enkelte filtreringstrin kan inddeles i f\u00f8lgende kategorier:  - URL-filter (kun for web-data)  - Linje-deduplikering  - Kvalitetsfilter  - Fjernelse af personoplysninger  - Dokument-deduplikering</p> <p>De enkelte trin er beskrevet i nedenst\u00e5ende afsnit. Efter filtreringstrinene bliver vores tekstdata tokenized, dvs. konverteret til et bin\u00e6rt format der kan l\u00e6ses af modellen.</p> <p></p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#url-filtrering","title":"URL-filtrering","text":"<p>Data som kommer fra offentlige hjemmesider og dermed har en URL som metadata, bliver f\u00f8rst processeret af et URL-filter.</p> <p>For alle dom\u00e6ner i datas\u00e6ttet hentes dom\u00e6nets robots.txt og ai.txt periodisk. Hvis disse ikke tillader CommonCrawl eller andre sprogmodel-crawlers tilf\u00f8jes dom\u00e6net til en blokeringsliste og dokumenter der stammer fra disse dom\u00e6ner filtreres v\u00e6k, selv om de p\u00e5g\u00e6ldende sider m\u00e5tte v\u00e6re hentet p\u00e5 et tidspunkt, hvor robots.txt/ai.txt ikke blokerede for denne type for crawling.</p> <p>Derudover anvendes blokeringslister fra forskellige offentligt tilg\u00e6ngelige databaser over skadeligt indhold. Vi bruger datatrove's indbyggede filter samt Dolma's samling af blokeringslister. Disse lister omfatter bl.a. f\u00f8lgende kategorier:</p> <ul> <li>Porno (b\u00e5de via lister og via ord der indg\u00e5r i dom\u00e6ne-navnet)</li> <li>Phishing</li> <li>Reklamer</li> <li>Kriminelle sider</li> <li>Abuse</li> <li>Fraud</li> <li>Malware</li> <li>Pirat</li> <li>Ransomware</li> <li>Scam</li> <li>Redirect</li> <li>Crypto</li> <li>Drugs</li> <li>Gambling</li> <li>Vaping</li> <li>Social Networks</li> </ul>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#deduplikering","title":"Deduplikering","text":"<p>Deduplikering anvendes til at fjerne gentagelser. Gentagelser i tr\u00e6ningsdata kan p\u00e5virke modellen i en u\u00f8nsket retning. Der anvendes to typer af deduplikering linje-deduplikering og dokument-deduplikering</p> <p>Linje-deduplikering er en proces hvor gentagne linjer fjernes p\u00e5 tv\u00e6rs af dokumenter. Dette er is\u00e6r anvendeligt p\u00e5 web-data, hvor f.eks. cookie notifikationer og menuer gentages p\u00e5 tv\u00e6rs af mange sider. Denne type af deduplikering implementeres effektivt vha. et s\u00e5kaldt Bloom filter. Visse typer af datas\u00e6t kan med fordel fritages for linje-deduplikering. F.eks. vil der i juridiske dokumenter ofte indg\u00e5 en r\u00e6kke standard formuleringer og deduplikering af disse kan \u00f8del\u00e6gge dokumenternes betydning.</p> <p>I dokument-deduplikering sammenlignes alle dokumenter p\u00e5 tv\u00e6rs af det rensede dokumentkorpus og dokumenter der indholdsm\u00e6ssigt er tilpas t\u00e6t p\u00e5 hinanden grupperes i en klynge. Fra hver klynge udtr\u00e6kkes \u00e9t enkelt dokument. P\u00e5 den m\u00e5de undg\u00e5s at visse dokumenter bliver overrepr\u00e6senteret i det endelige datas\u00e6t. </p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#kvalitetsfilter","title":"Kvalitetsfilter","text":"<p>Web-data kan indeholde meget st\u00f8j i form af stumper af HTML eller andet kode og ufuldst\u00e6ndige s\u00e6tninger. Der anvendes forskellige heuristikker, som er baseret p\u00e5 statistik for almindelig tekst, der fanger disse dokumenter af d\u00e5rlig kvalitet. Vi bruger p.t. samme filtre som Gopher og C4, men der unders\u00f8ges ogs\u00e5 mulighed for filtrering baseret p\u00e5 perpleksitet og andre metrikker.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#personhenfrbar-information","title":"Personhenf\u00f8rbar Information","text":"<p>N\u00e5r en model tr\u00e6nes p\u00e5 data, som indeholder personhenf\u00f8rbar information, medf\u00f8rer det en risiko for at modellen reproducerer denne information under k\u00f8rsler. S\u00e5 vidt det er muligt detekteres disse kategorier og erstattes med generiske erstatninger af samme type. Eksempler p\u00e5 personhenf\u00f8rbar information er: Navne, e-mails, telefonnumre, CPR-numre.</p> <p>En udfordring er at hverken menneskelig eller maskinel fjernelse af personhenf\u00f8rbar information er 100% n\u00f8jagtigt, s\u00e5 datas\u00e6t uden disse er at foretr\u00e6kke. </p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering/#dialog-om-data","title":"Dialog om data","text":"<p>Vi har den st\u00f8rste respekt for dem, der ejer data. Vi forst\u00e5r, hvor vigtigt det er at beskytte og respektere dataejeres \u00f8nsker om hvad og hvordan deres data m\u00e5 bruges til. </p> <p>Hvis du har nogen sp\u00f8rgsm\u00e5l vedr\u00f8rende de data vi bruger, er du altid velkommen til at kontakte os. Vi er meget \u00e5bne for dialog og s\u00e6tter pris p\u00e5 input, da det hj\u00e6lper os med at forbedre vores praksis og sikre, at vi lever op til dataejeres \u00f8nsker. </p> <p>Din feedback er vigtig for os, og vi ser frem til at h\u00f8re fra dig.</p>"},{"location":"blog/2024/07/04/datakilder/","title":"Datakilder","text":"<p>De data som sprogmodeller tr\u00e6nes p\u00e5 er afg\u00f8rende for hvad de kan bruges til. I Danish Foundation Models (DFM) er tilgangen at vi skal have sikkerhed for at vi m\u00e5 benytte de data vi tr\u00e6ner p\u00e5 fra data ejere, samt at vi har fokus p\u00e5 v\u00e6rdiskabende use-cases. Dette g\u00f8r vi blandt andet gennem samarbejdet med Dansk Sprogmodel Konsortium.</p>"},{"location":"blog/2024/07/04/datakilder/#nuvrende-datakilder","title":"Nuv\u00e6rende datakilder","text":"<p>Vi arbejder kontinuerligt p\u00e5 at indsamle data fra flere kilder. Nedenst\u00e5ende tabel indeholder kilder som lige nu, efter bedste overbevisning, kan anvendes til tr\u00e6ning af en dansk sprogmodel. M\u00e6ngden af data vi har nu, er ikke tilstr\u00e6kkelig til at tr\u00e6ne en dansk sprogmodel fra grunden. St\u00f8rrelsen er angivet i antal tegn.</p> Datas\u00e6t Dato Dom\u00e6ne Licens St\u00f8rrelse AI aktindsigt nutidig Kommunale hjemmesider CC0-1.0 408M Domsdatabasen 1855-nu Domme CC0-1.0 91.2M Eur-lex-sum-da 1993-nu Jura (EU) CC-BY-SA 4.0 87.8M FTSpeech 2017-nu Folketingets taler Ikke standard 244M Scrape Hovedstaden nutidig Sundhed CC0-1.0 79.9M MeMo 1870-1899 Sk\u00f8nlitteratur Offentligt Dom\u00e6ne 319M Wikipedia nutidig Encyklop\u00e6di CC-BY-SA 4.0 498M Retsinformation.dk (*) nutidig Lovtekster Ikke standard (*) 1.42G Skat.dk (*) nutidig Skatteinformation CC0-1.0 354M H-S\u00f8 (*) nutidig Retssager CC0-1.0 204 Hestenettet (*) nutidig Forum CC0-1.0 1.19G Folketinget (*) 2009-2019 Debat Ikke standard 351M Europarl (*) 2004-2008 Debat CC0-1.0 312M Spontaneous Speech (*) 2019 Samtaler CC0-1.0 4.0M NAAT (*) 1930-nu Taler CC0-1.0 881k Dansk Litteratur (*) 1700-nu Litteratur CC0-1.0 162M Gutenberg (*) 1700-nu Litteratur Ikke Standard 19.2M WikiBooks (*) 2019-2020 Manualer CC0-1.0 17.5M WikiSource (*) 1700-nu Litteratur CC0-1.0 15.5M Johannes V. Jensen (*) - JVJ\u2019s v\u00e6rker CC-BY-SA 4.0 10.7M Religi\u00f8se Tekster (*) - Religi\u00f8se CC0-1.0 3.56M TV2R (*) 2015-2019 Nyheder CC-BY 4.0 64.04M Dasem Data (*) nutidig Andet Ikke standard 4.45M Botxt (*) nutidig Bornholmsk CC0-1.0 2.01M DDT (*) nutidig Andet CC-BY-SA 4.0 546k S\u00f8nderjysk (*) nutidig S\u00f8nderjysk CC0-1.0 140k <p>Listen vil l\u00f8bende blive opdateret med flere datakilder. Data kommer bl.a. til at v\u00e6re fra samarbejdet med Dansk Sprogmodel Konsortium. Det skal bem\u00e6rkes at nogle af datas\u00e6ttene kommer fra Danish Gigaword, angivet i tabellen med (*).</p>"},{"location":"blog/2024/07/04/datakilder/#respekt-for-dataejere","title":"Respekt for dataejere","text":"<p>Vi har den st\u00f8rste respekt for dem, der ejer data. Vi forst\u00e5r, hvor vigtigt det er at beskytte og respektere dataejeres \u00f8nsker om hvad deres data m\u00e5 bruges til. Hvis du har nogen sp\u00f8rgsm\u00e5l vedr\u00f8rende de data vi bruger, er du altid velkommen til at kontakte os. Vi er meget \u00e5bne for dialog og s\u00e6tter pris p\u00e5 input, da det hj\u00e6lper os med at forbedre vores praksis og sikre, at vi lever op til dataejeres \u00f8nsker. Din feedback er vigtig for os, og vi ser frem til at h\u00f8re fra dig.</p>"},{"location":"datasheets/augmented_dagw/","title":"Augmented dagw","text":"<pre><code>pretty_name: Augmented_DAGW\ndescription: &gt;\n  This is an augmented version of the Danish Gigaword Dataset, which includes texts from several domains and forms. It integrates data from multiple sources, including Danish novels, Reddit comments, and news articles. Texts have undergone cleaning processes such as deduplication and URL filtering.\nlanguages:\n  - da\nlicense:\n  - cc-by-4.0\nmultilinguality:\n  - multilingual\nsize_categories:\n  - 10M&lt;n&lt;100M\ntask_categories:\n  - text-generation\n  - fill-mask\n  - summarization\ntask_ids:\n  - language-modeling\nconfigs:\n  - name: dagw\n    description: \"Filtered Danish Gigaword Corpus excluding Twitter data.\"\n    homepage: \"https://huggingface.co/datasets/DDSC/partial-danish-gigaword-no-twitter\"\n    size: \"511160 records\"\n    license: \"cc-by-4.0\"\n  - name: memo\n    description: \"Data from the MeMo corpus, includes normalized Danish novels from 1870-1899.\"\n    homepage: \"https://huggingface.co/datasets/MiMe-MeMo/Corpus-v1.1\"\n    size: \"858 records\"\n    license: \"cc-by-4.0\"\n  - name: scandi-reddit\n    description: \"Post-processed corpus of Reddit comments.\"\n    homepage: \"https://huggingface.co/datasets/alexandrainst/scandi-reddit\"\n    size: \"13479774 records\"\n    license: \"cc-by-4.0\"\n  - name: nordjylland-news\n    description: \"Dataset with text-summary pairs from Danish TV2 Nord news articles.\"\n    homepage: \"https://huggingface.co/datasets/alexandrainst/nordjylland-news-summarization\"\n    size: \"75219 records\"\n    license: \"cc-by-4.0\"\n</code></pre>"},{"location":"datasheets/augmented_dagw/#augmented-dagw","title":"Augmented DAGW","text":"<p>This is a quick introduction and summarization of an augmented Danish Gigaword Dataset. Here we will describe the different components and how we did the simple cleaning.</p>"},{"location":"datasheets/augmented_dagw/#components","title":"Components","text":"<p>Currently, the augmented DAGW consists of four components, each a smaller dataset on its own: 1. DAGW (Danish Gigaword): The Danish Gigaword Corpus contains text spanning several domains and forms. This version does not include the sections containing Tweets. 511160 records in total (after filtering) 2. MeMo: This is data release version 1.1 of the MeMo corpus comprising almost all Danish novels from the period 1870-1899, known as the Modern Breakthrough. Note that we only included the normalised corpus in the huggingface repository. 858 records in total. 3. Scandi-Reddit: ScandiReddit is a filtered and post-processed corpus consisting of comments from Reddit. 13479774 records in total. 4. Nordjylland-News: This dataset consists of pairs containing text and corresponding summaries extracted from the Danish newspaper TV2 Nord. Note that here we only included the training dataset. 75219 records in total.</p>"},{"location":"datasheets/augmented_dagw/#columnskeys","title":"Columns/Keys","text":"<p>All the records in the dataset have the following unified columns/keys:</p> <pre><code>{\n    \"id\": \"...\",             # MANDATORY: source-specific identifier\n    \"text\": \"...\",           # MANDATORY: textual content of the document\n    \"source\": \"...\",         # MANDATORY: source of the data, such as peS2o, common-crawl, etc.\n    \"added\": \"...\",          # OPTIONAL: timestamp ai2 acquired this data\n    \"created\": \"...\"         # OPTIONAL: timestamp when orig document was created (best-guess if not available)\n    \"metadata\": {...}        # OPTIONAL: source-specific metadata\n}\n</code></pre> <p>Moreover, each component has its own metadata which you can find in the following samples or the section: Metadata.</p>"},{"location":"datasheets/augmented_dagw/#samples-from-components","title":"Samples from components","text":"<p>Here are samples from each components (only the first 50 characters in <code>'text'</code> were printed out): - A sample from DAGW: <pre><code>{'text': \"J\u00d8RGINE J\u00d8RGINE K\u00d8BENHAVN HAGE &amp; CLAUSENS FORLAG (J. FR. CLAUSEN) 1926...\", 'id': 'jvj_J\u00f8rgine', 'added': 'Fri Jun 26 13:06:11 2020 CEST +0200', 'created': '1873-01-01T00:00:00.000Z, 1951-01-01T00:00:00.000Z', 'metadata': {'domain': 'Wiki &amp; Books', 'license': 'Attribution-ShareAlike 4.0 International', 'sub-source': 'Johannes V. Jensen (Danish poet)'}, 'source': 'dagw'}\n</code></pre> - A sample from MeMo: <pre><code>{'id': '130024227090', 'text': 'I . \\nI den fornemste gade l\u00e5 en pr\u00e6gtig , \\ngammeld...', 'source': 'KB', 'added': '2024-03-28T12:46:27.000Z', 'created': '1870-01-01T00:00:00.000Z, 1970-01-01T00:00:00.000Z', 'metadata': {'file_received': 'y', 'filename': '1870_AndersenHC_LykkePeer.pdf', 'firstname': 'H.C.', 'surname': 'Andersen', 'pseudonym': None, 'gender': 'm', 'nationality': 'dk', 'title': 'Lykke-Peer', 'subtitle': None, 'volume': None, 'year': 1870, 'pages': '183', 'illustrations': 'n', 'typeface': 'gothic', 'publisher': 'Reitzel', 'price': '2,25', 'file_status': 'Modtaget fra KB 7.4.2022 DEL2 sending 5', 'notes': \"OBS! PDF'en er ren tekst i antikva, men den fysiske bog formentlig fraktur. Det kalder p\u00e5 separate kolonner: pdf-typeface eller file-typeface og book-typeface. /PD\", 'filepath': None, 'fileformat': 'pdftxt', 'txt_received': 'y', 'readable': 'y', 'historical': 'n', 'period': None, 'period_notes': None, 'novel_start': '10', 'novel_end': '190', 'novelstart_rescan': None, 'novelend_rescan': None, 'start_end_page_notes': None, 'serialno': 12.0, 'quarantine': None, 'discard': None}}\n</code></pre> - A sample from Scandi-Reddit: <pre><code>{'text': 'Bergen er \u00f8delagt. Det er ikke moro mer....', 'id': '0', 'source': 'scandi-reddit', 'created': '2005-12-01T00:00:00.000Z, 2022-11-01T00:00:00.000Z', 'added': '2024-04-17T12:50:03.000Z', 'metadata': {'language': 'da', 'language_confidence': 0.7472341657, 'subreddit': 'Norway'}}\n</code></pre> - A sample from Nordjylland-News: <pre><code>{'id': 'nordjylland-news0', 'text': 'Opdatering: Manden er nu fundet af Nordjyllands Po...', 'source': 'TV2 Nord', 'added': '2024-03-15T14:35:19.000Z', 'created': '2024-03-15T14:35:19.000Z, 2025-03-15T14:35:19.000Z', 'metadata': {'summary': 'Nye oplysninger i sagen om en forsvunden mand har endnu en gang f\u00e5et politiet til at henvende sig til borgerne.', 'text_len': 1739, 'summary_len': 111}}\n</code></pre></p>"},{"location":"datasheets/augmented_dagw/#data-collection","title":"Data Collection","text":"<p>Here we list the python scripts for collecting, converting and compressing each sub-dataset.</p> <ul> <li>DAGW: https://github.com/centre-for-humanities-computing/danish-foundation-models/blob/main/data-processing/scripts/convert_dagw_to_jsonlgz.py</li> <li>MeMo: https://github.com/centre-for-humanities-computing/danish-foundation-models/blob/main/data-processing/scripts/memo/convert_memo_to_jsonlgz.py</li> <li>Scandi-Reddit: https://github.com/centre-for-humanities-computing/danish-foundation-models/blob/main/data-processing/scripts/convert_scandi_reddit_to_jsonlgz.py</li> <li>Nordjylland-News: https://github.com/centre-for-humanities-computing/danish-foundation-models/blob/main/data-processing/scripts/convert_nordjyllandnews_to_jsonlgz.py</li> </ul>"},{"location":"datasheets/augmented_dagw/#data-cleaning","title":"Data Cleaning","text":"<ol> <li>First, we filtered out all the empty documents (based on <code>\"text\"</code>: either empty or totally white space) in DAGW: 162051 out of 673211 empty docs found in total.</li> <li>After combining all 4 components, we applied deduplication and URL block list to the augmented DAGW following this filtering pipeline with Dolma.</li> </ol>"},{"location":"datasheets/dagw-adl/","title":"Dataset Card for  Archive for Danish Literature","text":""},{"location":"datasheets/dagw-adl/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 498</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-adl/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'SAMLEDE V\u00c6RKER\n\nJEPPE AAKJ\u00c6R GYLDENDALSKE BOGHANDE',\n    'source': 'dagw-adl',\n    'id': 'adl_aakjaer06val',\n    'added': '2020-09-14',\n    'created': '1700-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': ' Archive for Danish Literature'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-adl/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-adl/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-botxt/","title":"Dataset Card for Bornholmsk (Danish dialect)","text":""},{"location":"datasheets/dagw-botxt/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 106</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-botxt/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'R\u00e6ua-L\u00e2rs\n\nR\u00e6ua-L\u00e2rs \u00e5 hans Konna, Stina, bode uda',\n    'source': 'dagw-botxt',\n    'id': 'botxt_0000040',\n    'added': '2024-05-16',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Other',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Bornholmsk (Danish dialect)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-botxt/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-botxt/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-dannet/","title":"Dataset Card for DanNet (Danish WordNet)","text":""},{"location":"datasheets/dagw-dannet/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 49040</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-dannet/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'N\u00e5r fodboldholdet fra 1. division i Ikast spiller ',\n    'source': 'dagw-dannet',\n    'id': 'dannet_46506',\n    'added': '2020-09-24',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'dannet',\n        'license': 'Commercial Use of DanNet\n\nDanNet may be used in commercial applications in accordance with the following\nlicense agreement. An attorney representing the commercial interest should\nreview this DanNet license with respect to the intended use.\n\nDanNet 1.0 License\n\nDanNet Release 2.1\n\nThis software and database is being provided to you, the LICENSEE, by University\nof Copenhagen and Society for Danish Language and Literature under the following\nlicense. By obtaining, using and/or copying this software and database, you\nagree that you have read, understood, and will comply with these terms and\nconditions.\n\nPermission to use, copy, modify and distribute this software and database and\nits documentation for any purpose and without fee or royalty is hereby granted,\nprovided that you agree to comply with the following copyright notice and\nstatements, including the disclaimer, and that the same appear on ALL copies of\nthe software, database and documentation, including modifications that you make\nfor internal use or for distribution.\n\nTHIS SOFTWARE AND DATABASE IS PROVIDED \"AS IS\" AND UNIVERSITY OF COPENHAGEN and\nSOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO REPRESENTATIONS OR\nWARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION,\nUNIVERSITY OF COPENHAGEN AND SOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO\nREPRESENTATIONS OR WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR\nPURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL\nNOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS.\n\nThe names of University of Copenhagen and Society for Danish Language and\nLiterature may not be used in advertising or publicity pertaining to\ndistribution of the software and/or database. Title to copyright in this\nsoftware, database and any associated documentation shall at all times remain\nwith University of Copenhagen and Society for Danish Language and Literature and\nLICENSEE agrees to preserve same.\n\nDanNet 2.1 Copyright 2009-12 by University of Copenhagen and Society for Danish',\n        'source-pretty': 'DanNet (Danish WordNet)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-dannet/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-dannet/#license-information","title":"License Information","text":"DanNet 1.0 License <p> Commercial Use of DanNet  DanNet may be used in commercial applications in accordance with the following license agreement. An attorney representing the commercial interest should review this DanNet license with respect to the intended use.  DanNet 1.0 License  DanNet Release 2.1  This software and database is being provided to you, the LICENSEE, by University of Copenhagen and Society for Danish Language and Literature under the following license. By obtaining, using and/or copying this software and database, you agree that you have read, understood, and will comply with these terms and conditions.  Permission to use, copy, modify and distribute this software and database and its documentation for any purpose and without fee or royalty is hereby granted, provided that you agree to comply with the following copyright notice and statements, including the disclaimer, and that the same appear on ALL copies of the software, database and documentation, including modifications that you make for internal use or for distribution.  THIS SOFTWARE AND DATABASE IS PROVIDED \"AS IS\" AND UNIVERSITY OF COPENHAGEN and SOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, UNIVERSITY OF COPENHAGEN AND SOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO REPRESENTATIONS OR WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS.  The names of University of Copenhagen and Society for Danish Language and Literature may not be used in advertising or publicity pertaining to distribution of the software and/or database. Title to copyright in this software, database and any associated documentation shall at all times remain with University of Copenhagen and Society for Danish Language and Literature and LICENSEE agrees to preserve same.  DanNet 2.1 Copyright 2009-12 by University of Copenhagen and Society for Danish </p>"},{"location":"datasheets/dagw-depbank/","title":"Dataset Card for Danish Dependency Treebank","text":""},{"location":"datasheets/dagw-depbank/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 536</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-depbank/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'H.L. Hansen var en us\u00e6dvanmlig og frodig personlig',\n    'source': 'dagw-depbank',\n    'id': 'depbank_0375',\n    'added': '2024-05-16',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Other',\n        'license': 'Attribution-ShareAlike 4.0 International',\n        'source-pretty': 'Danish Dependency Treebank'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-depbank/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-depbank/#license-information","title":"License Information","text":"Creative Commons Attribution Share Alike 4.0 <p> Attribution-ShareAlike 4.0 International </p>"},{"location":"datasheets/dagw-ep/","title":"Dataset Card for European Parliament","text":""},{"location":"datasheets/dagw-ep/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 4213</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-ep/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'TALER 6703: Jeg har stemt for henstillingen om god',\n    'source': 'dagw-ep',\n    'id': 'ep_07-02-01-008',\n    'added': '2019-11-20',\n    'created': '2004-01-01, 2009-01-01',\n    'metadata': {\n        'domain': 'Conversation',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'European Parliament'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-ep/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-ep/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-ft/","title":"Dataset Card for Folketinget (Danish Parliament)","text":""},{"location":"datasheets/dagw-ft/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 1315</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-ft/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'TALER 50: M\u00f8det er \u00e5bnet. I dag er der f\u00f8lgende an',\n    'source': 'dagw-ft',\n    'id': 'ft_20121M100',\n    'added': '2021-03-28',\n    'created': '2009-01-01, 2019-01-01',\n    'metadata': {\n        'domain': 'Conversation',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Folketinget (Danish Parliament)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-ft/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-ft/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-gutenberg/","title":"Dataset Card for Gutenberg","text":""},{"location":"datasheets/dagw-gutenberg/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 66</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-gutenberg/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Afskriverens bem\u00e6rkninger: \u00c5benlyse trykfejl er re',\n    'source': 'dagw-gutenberg',\n    'id': 'gutenberg_43899',\n    'added': '2020-09-12',\n    'created': '1700-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': '*** START: FULL LICENSE ***\n\nTHE FULL PROJECT GUTENBERG LICENSE\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n\nTo protect the Project Gutenberg-tm mission of promoting the free\ndistribution of electronic works, by using or distributing this work\n(or any other work associated in any way with the phrase \"Project\nGutenberg\"), you agree to comply with all the terms of the Full Project\nGutenberg-tm License available with this file or online at\n  www.gutenberg.org/license.\n\n\nSection 1.  General Terms of Use and Redistributing Project Gutenberg-tm\nelectronic works\n\n1.A.  By reading or using any part of this Project Gutenberg-tm\nelectronic work, you indicate that you have read, understand, agree to\nand accept all the terms of this license and intellectual property\n(trademark/copyright) agreement.  If you do not agree to abide by all\nthe terms of this agreement, you must cease using and return or destroy\nall copies of Project Gutenberg-tm electronic works in your possession.\nIf you paid a fee for obtaining a copy of or access to a Project\nGutenberg-tm electronic work and you do not agree to be bound by the\nterms of this agreement, you may obtain a refund from the person or\nentity to whom you paid the fee as set forth in paragraph 1.E.8.\n\n1.B.  \"Project Gutenberg\" is a registered trademark.  It may only be\nused on or associated in any way with an electronic work by people who\nagree to be bound by the terms of this agreement.  There are a few\nthings that you can do with most Project Gutenberg-tm electronic works\neven without complying with the full terms of this agreement.  See\nparagraph 1.C below.  There are a lot of things you can do with Project\nGutenberg-tm electronic works if you follow the terms of this agreement\nand help preserve free future access to Project Gutenberg-tm electronic\nworks.  See paragraph 1.E below.\n\n1.C.  The Project Gutenberg Literary Archive Foundation (\"the Foundation\"\nor PGLAF), owns a compilation copyright in the collection of Project\nGutenberg-tm electronic works.  Nearly all the individual works in the\ncollection are in the public domain in the United States.  If an\nindividual work is in the public domain in the United States and you are\nlocated in the United States, we do not claim a right to prevent you from\ncopying, distributing, performing, displaying or creating derivative\nworks based on the work as long as all references to Project Gutenberg\nare removed.  Of course, we hope that you will support the Project\nGutenberg-tm mission of promoting free access to electronic works by\nfreely sharing Project Gutenberg-tm works in compliance with the terms of\nthis agreement for keeping the Project Gutenberg-tm name associated with\nthe work.  You can easily comply with the terms of this agreement by\nkeeping this work in the same format with its attached full Project\nGutenberg-tm License when you share it without charge with others.\n\n1.D.  The copyright laws of the place where you are located also govern\nwhat you can do with this work.  Copyright laws in most countries are in\na constant state of change.  If you are outside the United States, check\nthe laws of your country in addition to the terms of this agreement\nbefore downloading, copying, displaying, performing, distributing or\ncreating derivative works based on this work or any other Project\nGutenberg-tm work.  The Foundation makes no representations concerning\nthe copyright status of any work in any country outside the United\nStates.\n\n1.E.  Unless you have removed all references to Project Gutenberg:\n\n1.E.1.  The following sentence, with active links to, or other immediate\naccess to, the full Project Gutenberg-tm License must appear prominently\nwhenever any copy of a Project Gutenberg-tm work (any work on which the\nphrase \"Project Gutenberg\" appears, or with which the phrase \"Project\nGutenberg\" is associated) is accessed, displayed, performed, viewed,\ncopied or distributed:\n\nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.org\n\n1.E.2.  If an individual Project Gutenberg-tm electronic work is derived\nfrom the public domain (does not contain a notice indicating that it is\nposted with permission of the copyright holder), the work can be copied\nand distributed to anyone in the United States without paying any fees\nor charges.  If you are redistributing or providing access to a work\nwith the phrase \"Project Gutenberg\" associated with or appearing on the\nwork, you must comply either with the requirements of paragraphs 1.E.1\nthrough 1.E.7 or obtain permission for the use of the work and the\nProject Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or\n1.E.9.\n\n1.E.3.  If an individual Project Gutenberg-tm electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any additional\nterms imposed by the copyright holder.  Additional terms will be linked\nto the Project Gutenberg-tm License for all works posted with the\npermission of the copyright holder found at the beginning of this work.\n\n1.E.4.  Do not unlink or detach or remove the full Project Gutenberg-tm\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg-tm.\n\n1.E.5.  Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg-tm License.\n\n1.E.6.  You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including any\nword processing or hypertext form.  However, if you provide access to or\ndistribute copies of a Project Gutenberg-tm work in a format other than\n\"Plain Vanilla ASCII\" or other format used in the official version\nposted on the official Project Gutenberg-tm web site (www.gutenberg.org),\nyou must, at no additional cost, fee or expense to the user, provide a\ncopy, a means of exporting a copy, or a means of obtaining a copy upon\nrequest, of the work in its original \"Plain Vanilla ASCII\" or other\nform.  Any alternate format must include the full Project Gutenberg-tm\nLicense as specified in paragraph 1.E.1.\n\n1.E.7.  Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg-tm works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8.  You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg-tm electronic works provided\nthat\n\n- You pay a royalty fee of 20% of the gross profits you derive from\n     the use of Project Gutenberg-tm works calculated using the method\n     you already use to calculate your applicable taxes.  The fee is\n     owed to the owner of the Project Gutenberg-tm trademark, but he\n     has agreed to donate royalties under this paragraph to the\n     Project Gutenberg Literary Archive Foundation.  Royalty payments\n     must be paid within 60 days following each date on which you\n     prepare (or are legally required to prepare) your periodic tax\n     returns.  Royalty payments should be clearly marked as such and\n     sent to the Project Gutenberg Literary Archive Foundation at the\n     address specified in Section 4, \"Information about donations to\n     the Project Gutenberg Literary Archive Foundation.\"\n\n- You provide a full refund of any money paid by a user who notifies\n     you in writing (or by e-mail) within 30 days of receipt that s/he\n     does not agree to the terms of the full Project Gutenberg-tm\n     License.  You must require such a user to return or\n     destroy all copies of the works possessed in a physical medium\n     and discontinue all use of and all access to other copies of\n     Project Gutenberg-tm works.\n\n- You provide, in accordance with paragraph 1.F.3, a full refund of any\n     money paid for a work or a replacement copy, if a defect in the\n     electronic work is discovered and reported to you within 90 days\n     of receipt of the work.\n\n- You comply with all other terms of this agreement for free\n     distribution of Project Gutenberg-tm works.\n\n1.E.9.  If you wish to charge a fee or distribute a Project Gutenberg-tm\nelectronic work or group of works on different terms than are set\nforth in this agreement, you must obtain permission in writing from\nboth the Project Gutenberg Literary Archive Foundation and Michael\nHart, the owner of the Project Gutenberg-tm trademark.  Contact the\nFoundation as set forth in Section 3 below.\n\n1.F.\n\n1.F.1.  Project Gutenberg volunteers and employees expend considerable\neffort to identify, do copyright research on, transcribe and proofread\npublic domain works in creating the Project Gutenberg-tm\ncollection.  Despite these efforts, Project Gutenberg-tm electronic\nworks, and the medium on which they may be stored, may contain\n\"Defects,\" such as, but not limited to, incomplete, inaccurate or\ncorrupt data, transcription errors, a copyright or other intellectual\nproperty infringement, a defective or damaged disk or other medium, a\ncomputer virus, or computer codes that damage or cannot be read by\nyour equipment.\n\n1.F.2.  LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right\nof Replacement or Refund\" described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg-tm trademark, and any other party distributing a Project\nGutenberg-tm electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees.  YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3.  YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3.  LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from.  If you\nreceived the work on a physical medium, you must return the medium with\nyour written explanation.  The person or entity that provided you with\nthe defective work may elect to provide a replacement copy in lieu of a\nrefund.  If you received the work electronically, the person or entity\nproviding it to you may choose to give you a second opportunity to\nreceive the work electronically in lieu of a refund.  If the second copy\nis also defective, you may demand a refund in writing without further\nopportunities to fix the problem.\n\n1.F.4.  Except for the limited right of replacement or refund set forth\nin paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO OTHER\nWARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\nWARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n\n1.F.5.  Some states do not allow disclaimers of certain implied\nwarranties or the exclusion or limitation of certain types of damages.\nIf any disclaimer or limitation set forth in this agreement violates the\nlaw of the state applicable to this agreement, the agreement shall be\ninterpreted to make the maximum disclaimer or limitation permitted by\nthe applicable state law.  The invalidity or unenforceability of any\nprovision of this agreement shall not void the remaining provisions.\n\n1.F.6.  INDEMNITY - You agree to indemnify and hold the Foundation, the\ntrademark owner, any agent or employee of the Foundation, anyone\nproviding copies of Project Gutenberg-tm electronic works in accordance\nwith this agreement, and any volunteers associated with the production,\npromotion and distribution of Project Gutenberg-tm electronic works,\nharmless from all liability, costs and expenses, including legal fees,\nthat arise directly or indirectly from any of the following which you do\nor cause to occur: (a) distribution of this or any Project Gutenberg-tm\nwork, (b) alteration, modification, or additions or deletions to any\nProject Gutenberg-tm work, and (c) any Defect you cause.\n\n\nSection  2.  Information about the Mission of Project Gutenberg-tm\n\nProject Gutenberg-tm is synonymous with the free distribution of\nelectronic works in formats readable by the widest variety of computers\nincluding obsolete, old, middle-aged and new computers.  It exists\nbecause of the efforts of hundreds of volunteers and donations from\npeople in all walks of life.\n\nVolunteers and financial support to provide volunteers with the\nassistance they need are critical to reaching Project Gutenberg-tm's\ngoals and ensuring that the Project Gutenberg-tm collection will\nremain freely available for generations to come.  In 2001, the Project\nGutenberg Literary Archive Foundation was created to provide a secure\nand permanent future for Project Gutenberg-tm and future generations.\nTo learn more about the Project Gutenberg Literary Archive Foundation\nand how your efforts and donations can help, see Sections 3 and 4\nand the Foundation information page at www.gutenberg.org\n\n\nSection 3.  Information about the Project Gutenberg Literary Archive\nFoundation\n\nThe Project Gutenberg Literary Archive Foundation is a non profit\n501(c)(3) educational corporation organized under the laws of the\nstate of Mississippi and granted tax exempt status by the Internal\nRevenue Service.  The Foundation's EIN or federal tax identification\nnumber is 64-6221541.  Contributions to the Project Gutenberg\nLiterary Archive Foundation are tax deductible to the full extent\npermitted by U.S. federal laws and your state's laws.\n\nThe Foundation's principal office is located at 4557 Melan Dr. S.\nFairbanks, AK, 99712., but its volunteers and employees are scattered\nthroughout numerous locations.  Its business office is located at 809\nNorth 1500 West, Salt Lake City, UT 84116, (801) 596-1887.  Email\ncontact links and up to date contact information can be found at the\nFoundation's web site and official page at www.gutenberg.org/contact\n\nFor additional contact information:\n     Dr. Gregory B. Newby\n     Chief Executive and Director\n     gbnewby@pglaf.org\n\nSection 4.  Information about Donations to the Project Gutenberg\nLiterary Archive Foundation\n\nProject Gutenberg-tm depends upon and cannot survive without wide\nspread public support and donations to carry out its mission of\nincreasing the number of public domain and licensed works that can be\nfreely distributed in machine readable form accessible by the widest\narray of equipment including outdated equipment.  Many small donations\n($1 to $5,000) are particularly important to maintaining tax exempt\nstatus with the IRS.\n\nThe Foundation is committed to complying with the laws regulating\ncharities and charitable donations in all 50 states of the United\nStates.  Compliance requirements are not uniform and it takes a\nconsiderable effort, much paperwork and many fees to meet and keep up\nwith these requirements.  We do not solicit donations in locations\nwhere we have not received written confirmation of compliance.  To\nSEND DONATIONS or determine the status of compliance for any\nparticular state visit www.gutenberg.org/donate\n\nWhile we cannot and do not solicit contributions from states where we\nhave not met the solicitation requirements, we know of no prohibition\nagainst accepting unsolicited donations from donors in such states who\napproach us with offers to donate.\n\nInternational donations are gratefully accepted, but we cannot make\nany statements concerning tax treatment of donations received from\noutside the United States.  U.S. laws alone swamp our small staff.\n\nPlease check the Project Gutenberg Web pages for current donation\nmethods and addresses.  Donations are accepted in a number of other\nways including checks, online payments and credit card donations.\nTo donate, please visit:  www.gutenberg.org/donate\n\n\nSection 5.  General Information About Project Gutenberg-tm electronic\nworks.\n\nProfessor Michael S. Hart was the originator of the Project Gutenberg-tm\nconcept of a library of electronic works that could be freely shared\nwith anyone.  For forty years, he produced and distributed Project\nGutenberg-tm eBooks with only a loose network of volunteer support.\n\nProject Gutenberg-tm eBooks are often created from several printed\neditions, all of which are confirmed as Public Domain in the U.S.\nunless a copyright notice is included.  Thus, we do not necessarily\nkeep eBooks in compliance with any particular paper edition.\n\nMost people start at our Web site which has the main PG search facility:\n\n     www.gutenberg.org\n\nThis Web site includes information about Project Gutenberg-tm,\nincluding how to make donations to the Project Gutenberg Literary\nArchive Foundation, how to help produce our new eBooks, and how to\nsubscribe to our email newsletter to hear about new eBooks.\n',\n        'source-pretty': 'Gutenberg'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-gutenberg/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-gutenberg/#license-information","title":"License Information","text":"Gutenberg License <p> *** START: FULL LICENSE ***  THE FULL PROJECT GUTENBERG LICENSE PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK  To protect the Project Gutenberg-tm mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase \"Project Gutenberg\"), you agree to comply with all the terms of the Full Project Gutenberg-tm License available with this file or online at   www.gutenberg.org/license.   Section 1.  General Terms of Use and Redistributing Project Gutenberg-tm electronic works  1.A.  By reading or using any part of this Project Gutenberg-tm electronic work, you indicate that you have read, understand, agree to and accept all the terms of this license and intellectual property (trademark/copyright) agreement.  If you do not agree to abide by all the terms of this agreement, you must cease using and return or destroy all copies of Project Gutenberg-tm electronic works in your possession. If you paid a fee for obtaining a copy of or access to a Project Gutenberg-tm electronic work and you do not agree to be bound by the terms of this agreement, you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1.E.8.  1.B.  \"Project Gutenberg\" is a registered trademark.  It may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement.  There are a few things that you can do with most Project Gutenberg-tm electronic works even without complying with the full terms of this agreement.  See paragraph 1.C below.  There are a lot of things you can do with Project Gutenberg-tm electronic works if you follow the terms of this agreement and help preserve free future access to Project Gutenberg-tm electronic works.  See paragraph 1.E below.  1.C.  The Project Gutenberg Literary Archive Foundation (\"the Foundation\" or PGLAF), owns a compilation copyright in the collection of Project Gutenberg-tm electronic works.  Nearly all the individual works in the collection are in the public domain in the United States.  If an individual work is in the public domain in the United States and you are located in the United States, we do not claim a right to prevent you from copying, distributing, performing, displaying or creating derivative works based on the work as long as all references to Project Gutenberg are removed.  Of course, we hope that you will support the Project Gutenberg-tm mission of promoting free access to electronic works by freely sharing Project Gutenberg-tm works in compliance with the terms of this agreement for keeping the Project Gutenberg-tm name associated with the work.  You can easily comply with the terms of this agreement by keeping this work in the same format with its attached full Project Gutenberg-tm License when you share it without charge with others.  1.D.  The copyright laws of the place where you are located also govern what you can do with this work.  Copyright laws in most countries are in a constant state of change.  If you are outside the United States, check the laws of your country in addition to the terms of this agreement before downloading, copying, displaying, performing, distributing or creating derivative works based on this work or any other Project Gutenberg-tm work.  The Foundation makes no representations concerning the copyright status of any work in any country outside the United States.  1.E.  Unless you have removed all references to Project Gutenberg:  1.E.1.  The following sentence, with active links to, or other immediate access to, the full Project Gutenberg-tm License must appear prominently whenever any copy of a Project Gutenberg-tm work (any work on which the phrase \"Project Gutenberg\" appears, or with which the phrase \"Project Gutenberg\" is associated) is accessed, displayed, performed, viewed, copied or distributed:  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org  1.E.2.  If an individual Project Gutenberg-tm electronic work is derived from the public domain (does not contain a notice indicating that it is posted with permission of the copyright holder), the work can be copied and distributed to anyone in the United States without paying any fees or charges.  If you are redistributing or providing access to a work with the phrase \"Project Gutenberg\" associated with or appearing on the work, you must comply either with the requirements of paragraphs 1.E.1 through 1.E.7 or obtain permission for the use of the work and the Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or 1.E.9.  1.E.3.  If an individual Project Gutenberg-tm electronic work is posted with the permission of the copyright holder, your use and distribution must comply with both paragraphs 1.E.1 through 1.E.7 and any additional terms imposed by the copyright holder.  Additional terms will be linked to the Project Gutenberg-tm License for all works posted with the permission of the copyright holder found at the beginning of this work.  1.E.4.  Do not unlink or detach or remove the full Project Gutenberg-tm License terms from this work, or any files containing a part of this work or any other work associated with Project Gutenberg-tm.  1.E.5.  Do not copy, display, perform, distribute or redistribute this electronic work, or any part of this electronic work, without prominently displaying the sentence set forth in paragraph 1.E.1 with active links or immediate access to the full terms of the Project Gutenberg-tm License.  1.E.6.  You may convert to and distribute this work in any binary, compressed, marked up, nonproprietary or proprietary form, including any word processing or hypertext form.  However, if you provide access to or distribute copies of a Project Gutenberg-tm work in a format other than \"Plain Vanilla ASCII\" or other format used in the official version posted on the official Project Gutenberg-tm web site (www.gutenberg.org), you must, at no additional cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original \"Plain Vanilla ASCII\" or other form.  Any alternate format must include the full Project Gutenberg-tm License as specified in paragraph 1.E.1.  1.E.7.  Do not charge a fee for access to, viewing, displaying, performing, copying or distributing any Project Gutenberg-tm works unless you comply with paragraph 1.E.8 or 1.E.9.  1.E.8.  You may charge a reasonable fee for copies of or providing access to or distributing Project Gutenberg-tm electronic works provided that  - You pay a royalty fee of 20% of the gross profits you derive from      the use of Project Gutenberg-tm works calculated using the method      you already use to calculate your applicable taxes.  The fee is      owed to the owner of the Project Gutenberg-tm trademark, but he      has agreed to donate royalties under this paragraph to the      Project Gutenberg Literary Archive Foundation.  Royalty payments      must be paid within 60 days following each date on which you      prepare (or are legally required to prepare) your periodic tax      returns.  Royalty payments should be clearly marked as such and      sent to the Project Gutenberg Literary Archive Foundation at the      address specified in Section 4, \"Information about donations to      the Project Gutenberg Literary Archive Foundation.\"  - You provide a full refund of any money paid by a user who notifies      you in writing (or by e-mail) within 30 days of receipt that s/he      does not agree to the terms of the full Project Gutenberg-tm      License.  You must require such a user to return or      destroy all copies of the works possessed in a physical medium      and discontinue all use of and all access to other copies of      Project Gutenberg-tm works.  - You provide, in accordance with paragraph 1.F.3, a full refund of any      money paid for a work or a replacement copy, if a defect in the      electronic work is discovered and reported to you within 90 days      of receipt of the work.  - You comply with all other terms of this agreement for free      distribution of Project Gutenberg-tm works.  1.E.9.  If you wish to charge a fee or distribute a Project Gutenberg-tm electronic work or group of works on different terms than are set forth in this agreement, you must obtain permission in writing from both the Project Gutenberg Literary Archive Foundation and Michael Hart, the owner of the Project Gutenberg-tm trademark.  Contact the Foundation as set forth in Section 3 below.  1.F.  1.F.1.  Project Gutenberg volunteers and employees expend considerable effort to identify, do copyright research on, transcribe and proofread public domain works in creating the Project Gutenberg-tm collection.  Despite these efforts, Project Gutenberg-tm electronic works, and the medium on which they may be stored, may contain \"Defects,\" such as, but not limited to, incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other medium, a computer virus, or computer codes that damage or cannot be read by your equipment.  1.F.2.  LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right of Replacement or Refund\" described in paragraph 1.F.3, the Project Gutenberg Literary Archive Foundation, the owner of the Project Gutenberg-tm trademark, and any other party distributing a Project Gutenberg-tm electronic work under this agreement, disclaim all liability to you for damages, costs and expenses, including legal fees.  YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE PROVIDED IN PARAGRAPH 1.F.3.  YOU AGREE THAT THE FOUNDATION, THE TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGE.  1.F.3.  LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a defect in this electronic work within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending a written explanation to the person you received the work from.  If you received the work on a physical medium, you must return the medium with your written explanation.  The person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund.  If you received the work electronically, the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund.  If the second copy is also defective, you may demand a refund in writing without further opportunities to fix the problem.  1.F.4.  Except for the limited right of replacement or refund set forth in paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.  1.F.5.  Some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages. If any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement, the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law.  The invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions.  1.F.6.  INDEMNITY - You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause.   Section  2.  Information about the Mission of Project Gutenberg-tm  Project Gutenberg-tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers.  It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.  Volunteers and financial support to provide volunteers with the assistance they need are critical to reaching Project Gutenberg-tm's goals and ensuring that the Project Gutenberg-tm collection will remain freely available for generations to come.  In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg-tm and future generations. To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation information page at www.gutenberg.org   Section 3.  Information about the Project Gutenberg Literary Archive Foundation  The Project Gutenberg Literary Archive Foundation is a non profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service.  The Foundation's EIN or federal tax identification number is 64-6221541.  Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state's laws.  The Foundation's principal office is located at 4557 Melan Dr. S. Fairbanks, AK, 99712., but its volunteers and employees are scattered throughout numerous locations.  Its business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887.  Email contact links and up to date contact information can be found at the Foundation's web site and official page at www.gutenberg.org/contact  For additional contact information:      Dr. Gregory B. Newby      Chief Executive and Director      gbnewby@pglaf.org  Section 4.  Information about Donations to the Project Gutenberg Literary Archive Foundation  Project Gutenberg-tm depends upon and cannot survive without wide spread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment.  Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS.  The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States.  Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements.  We do not solicit donations in locations where we have not received written confirmation of compliance.  To SEND DONATIONS or determine the status of compliance for any particular state visit www.gutenberg.org/donate  While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate.  International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States.  U.S. laws alone swamp our small staff.  Please check the Project Gutenberg Web pages for current donation methods and addresses.  Donations are accepted in a number of other ways including checks, online payments and credit card donations. To donate, please visit:  www.gutenberg.org/donate   Section 5.  General Information About Project Gutenberg-tm electronic works.  Professor Michael S. Hart was the originator of the Project Gutenberg-tm concept of a library of electronic works that could be freely shared with anyone.  For forty years, he produced and distributed Project Gutenberg-tm eBooks with only a loose network of volunteer support.  Project Gutenberg-tm eBooks are often created from several printed editions, all of which are confirmed as Public Domain in the U.S. unless a copyright notice is included.  Thus, we do not necessarily keep eBooks in compliance with any particular paper edition.  Most people start at our Web site which has the main PG search facility:       www.gutenberg.org  This Web site includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks.  </p>"},{"location":"datasheets/dagw-hest/","title":"Dataset Card for Hestenettet (Danish debate forum)","text":""},{"location":"datasheets/dagw-hest/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 14391</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-hest/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Er den ikke k\u00e6r? \nJeg kan ikke forst\u00e5 at der altid',\n    'source': 'dagw-hest',\n    'id': 'hest_forum112802271280227_0',\n    'added': '2020-10-05',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Social Media',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Hestenettet (Danish debate forum)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-hest/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-hest/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-jvj/","title":"Dataset Card for Johannes V. Jensen (Danish poet)","text":""},{"location":"datasheets/dagw-jvj/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 42</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-jvj/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'J\u00d8RGINE J\u00d8RGINE K\u00d8BENHAVN HAGE &amp; CLAUSENS FORLAG (',\n    'source': 'dagw-jvj',\n    'id': 'jvj_J\u00f8rgine',\n    'added': '2020-06-26',\n    'created': '1873-01-01, 1951-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Attribution-ShareAlike 4.0 International',\n        'source-pretty': 'Johannes V. Jensen (Danish poet)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-jvj/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-jvj/#license-information","title":"License Information","text":"Creative Commons Attribution Share Alike 4.0 <p> Attribution-ShareAlike 4.0 International </p>"},{"location":"datasheets/dagw-naat/","title":"Dataset Card for NAAT","text":""},{"location":"datasheets/dagw-naat/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 129</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-naat/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Naar jeg i aften sender min nytaarshilsen til det ',\n    'source': 'dagw-naat',\n    'id': 'naat_1958kongfrederikix',\n    'added': '2020-02-11',\n    'created': '1930-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Conversation',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'NAAT'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-naat/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-naat/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-relig/","title":"Dataset Card for Religious texts","text":""},{"location":"datasheets/dagw-relig/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 66</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-relig/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Salomos H\u00f8jsang\nKys mig, giv mig Kys af din mund t',\n    'source': 'dagw-relig',\n    'id': 'relig_SON',\n    'added': '2020-09-14',\n    'created': '1700-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Religious texts'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-relig/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-relig/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-retsinformationdk/","title":"Dataset Card for retsinformation.dk (Danish legal information)","text":""},{"location":"datasheets/dagw-retsinformationdk/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 64043</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-retsinformationdk/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Den fulde tekst Pressen\u00e6vnets kendelse i sag nr. 1',\n    'source': 'dagw-retsinformationdk',\n    'id': 'retsinformationdk_173889',\n    'added': '2019-11-22',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Legal',\n        'license': 'Danish Copyright law at https://www.retsinformation.dk/forms/r0710.aspx?id=164796 states \n\n \u00a7 9. Love, administrative forskrifter, retsafg\u00f8relser og lignende offentlige aktstykker er ikke genstand for ophavsret.\n\nStk. 2. Bestemmelsen i stk. 1 g\u00e6lder ikke for v\u00e6rker, der fremtr\u00e6der som selvst\u00e6ndige bidrag i de i stk. 1 n\u00e6vnte aktstykker. S\u00e5danne v\u00e6rker m\u00e5 dog gengives i forbindelse med aktstykket. Retten til videre udnyttelse afh\u00e6nger af de i \u00f8vrigt g\u00e6ldende regler.\n',\n        'source-pretty': 'retsinformation.dk (Danish legal information)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-retsinformationdk/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-retsinformationdk/#license-information","title":"License Information","text":"Danish Copyright Law <p> Danish Copyright law at https://www.retsinformation.dk/forms/r0710.aspx?id=164796 states    \u00a7 9. Love, administrative forskrifter, retsafg\u00f8relser og lignende offentlige aktstykker er ikke genstand for ophavsret.  Stk. 2. Bestemmelsen i stk. 1 g\u00e6lder ikke for v\u00e6rker, der fremtr\u00e6der som selvst\u00e6ndige bidrag i de i stk. 1 n\u00e6vnte aktstykker. S\u00e5danne v\u00e6rker m\u00e5 dog gengives i forbindelse med aktstykket. Retten til videre udnyttelse afh\u00e6nger af de i \u00f8vrigt g\u00e6ldende regler.  </p>"},{"location":"datasheets/dagw-retspraksis/","title":"Dataset Card for retspraksis (Danish legal information)","text":""},{"location":"datasheets/dagw-retspraksis/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 4413</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-retspraksis/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'h\u00f8jesterets dom\n                        afsagt tor',\n    'source': 'dagw-retspraksis',\n    'id': 'retspraksis_517',\n    'added': '2020-09-24',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Legal',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'retspraksis (Danish legal information)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-retspraksis/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-retspraksis/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-skat/","title":"Dataset Card for Skat (Danish tax authority)","text":""},{"location":"datasheets/dagw-skat/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 14716</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-skat/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Andelsboligforeningers levering af brugsrettighede',\n    'source': 'dagw-skat',\n    'id': 'skat_SKM2010.712.SKAT',\n    'added': '2020-10-01',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Legal',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Skat (Danish tax authority)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-skat/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-skat/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-spont/","title":"Dataset Card for Spontaneous speech","text":""},{"location":"datasheets/dagw-spont/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 411</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-spont/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Taler 6: mm\nTaler 7: er du klar?\nTaler 6: ja\nTaler',\n    'source': 'dagw-spont',\n    'id': 'spont_PuzzleOfDanish132',\n    'added': '2020-01-21',\n    'created': '2019-01-01, 2020-01-01',\n    'metadata': {\n        'domain': 'Conversation',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Spontaneous speech'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-spont/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-spont/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-synne/","title":"Dataset Card for Synderjysk (Danish dialect)","text":""},{"location":"datasheets/dagw-synne/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 178</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-synne/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Mange\u00e6gskage Hent printvenligt dokument her \u2013 Klik',\n    'source': 'dagw-synne',\n    'id': 'synne_forening_0140',\n    'added': '2020-06-26',\n    'created': '2000-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Other',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Synderjysk (Danish dialect)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-synne/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-synne/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-tv2r/","title":"Dataset Card for TV 2 Radio (Danish news)","text":""},{"location":"datasheets/dagw-tv2r/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 49137</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-tv2r/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Storken er landet\n02 april 2017 kl. 17.58\nS\u00f8ndag a',\n    'source': 'dagw-tv2r',\n    'id': 'tv2r_92548',\n    'added': '2019-11-13',\n    'created': '2015-01-01, 2020-01-01',\n    'metadata': {\n        'domain': 'News',\n        'license': 'The owner of this content is TV2 Regionerne, Denmark.\nCreative Commons Attribution 4.0 International',\n        'source-pretty': 'TV 2 Radio (Danish news)'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-tv2r/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-tv2r/#license-information","title":"License Information","text":"Creative Commons Attribution Share Alike 4.0 <p> The owner of this content is TV2 Regionerne, Denmark. Creative Commons Attribution 4.0 International </p>"},{"location":"datasheets/dagw-wiki/","title":"Dataset Card for Wikipedia","text":""},{"location":"datasheets/dagw-wiki/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 264502</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-wiki/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Vimoutiers er en kommune i departementet Orne i Ba',\n    'source': 'dagw-wiki',\n    'id': 'wiki_366127',\n    'added': '2021-03-28',\n    'created': '2019-01-01, 2021-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Wikipedia'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-wiki/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-wiki/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-wikibooks/","title":"Dataset Card for Wikibooks","text":""},{"location":"datasheets/dagw-wikibooks/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 1319</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-wikibooks/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': 'Spilinfo.\nSpillet er lavet af Blizzard Entertainme',\n    'source': 'dagw-wikibooks',\n    'id': 'wikibooks_1125',\n    'added': '2021-03-28',\n    'created': '2019-01-01, 2021-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Wikibooks'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-wikibooks/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-wikibooks/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/dagw-wikisource/","title":"Dataset Card for Wikisource","text":""},{"location":"datasheets/dagw-wikisource/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 2429</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/dagw-wikisource/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'text': '&amp;lt;poem&amp;gt;\nK\u00e6mpeh\u00f8jen.\nJeg har st\u00e5et p\u00e5 mindets ',\n    'source': 'dagw-wikisource',\n    'id': 'wikisource_4804',\n    'added': '2021-03-28',\n    'created': '1700-01-01, 2022-01-01',\n    'metadata': {\n        'domain': 'Wiki &amp; Books',\n        'license': 'Creative Commons Legal Code\n\nCC0 1.0 Universal',\n        'source-pretty': 'Wikisource'\n        }\n}\n</code></pre></p>"},{"location":"datasheets/dagw-wikisource/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/dagw-wikisource/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/danews/","title":"DaNews","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>DaNews consists of articles from Danish news and tabloid media from 1 December 2019 to  30 April 2021. The articles stem from multiple news sources, including both online of physical newspapers.</p> <p>DaNews consists of 403 million tokens 93% were left after  quality filtering and deduplication.</p>"},{"location":"datasheets/danews/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [5] we add the following datasheet. </p>"},{"location":"datasheets/danews/#motivation","title":"Motivation","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>DANews was collected as a part of the HOPE project, examining news coverage during the COVID-19 pandemic. The purpose was to train a model to understand how the novelty and resonance imprint of COVID-19 as a case of crisis compared to non-crises news imprints.</p> <p>Any other comments?</p> <p>No.</p>"},{"location":"datasheets/danews/#composition","title":"Composition","text":"<p>How many instances are there in total (of each type, if appropriate)?</p> <p>The unfiltered dataset consists of 713 429 documents including a total of 403 089 625 tokens.</p> <p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances of the dataset are Danish articles derived from Danish tabloids or news media. </p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>Prior to filtering DaNews dataset contains all digitized news articles from the given period across the sources.</p> <p>What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.</p> <p>Each instance consists of the following columns <pre><code>'ArticleUrl', 'Heading', 'SubHeading', 'Lead', 'Paragraph', 'PublishDate', 'BodyText', \n'Captions', 'Authors', 'Source', 'WordCount', 'ArticleId', 'PageIds', 'Section', 'text'\n</code></pre></p> <p>Where we constructed the columns <code>text</code> column by joining the <code>Heading</code>, <code>SubHeading</code> using newline. If the text field is empty it is ignored and no newline is added. The we join the resulting string with the <code>BodyText</code> using two newlines.</p> <p>During the quality filtering, we add the following indicator columns: <pre><code>'passed_quality_filter', 'filtered_by_max_chr_length', 'filtered_by_doc_length', \n'filtered_by_mean_word_length', 'filtered_by_alpha_ratio', 'filtered_by_stop_word', \n'filtered_by_symbol_2_word_hashtag', 'filtered_by_symbol_2_word_ellipsis',\n'filtered_by_line_bullets_or_ellipsis', 'filtered_by_duplicate_lines_chr_fraction',\n'filtered_by_duplicate_paragraph_chr_fraction', 'filtered_by_top_ngram_chr_fraction',\n'filtered_by_duplicate_ngram_chr_fraction', 'is_duplicate'\n</code></pre></p> <p>Is there a label or target associated with each instance? If so, please provide a description.</p> <p>No.</p> <p>Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information but might include, e.g., redacted text.</p> <p>The team of researchers at the Humanities Computing Aarhus (CHCAA) have not removed any information from the instances.</p> <p>Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, and social network links)? If so, please describe how these relationships are made explicit.</p> <p>The metadata columns denote the relationship between articles including the date of publication, sections, and authors.</p> <p>Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p> <p>There are not splits performed on this dataset.</p> <p>Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</p> <p>News sources can publish their content both in an online and printed format which would lead to similar instances in the dataset. To alleviate this redundancy by removing near-duplicates (see Preprocessing/cleaning/labeling).</p> <p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?</p> <p>Articles are intended to tell a self-contained story but can include external references such as tweets or website URLs.</p> <p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</p> <p>Articles often describe content that is considered offensive, insulting, or threatening. </p>"},{"location":"datasheets/danews/#collection-process","title":"Collection Process","text":"<p>What mechanisms or procedures were used to collect the data (e.g., hardware  apparatuses or sensors, manual human curation, software programs, software APIs)?</p> <p>A team of researchers at the Center for Humanities Computing Aarhus (CHCAA) obtained this  dataset using a third-party API as well as a manual transfer from one of the parties. The API was limited  to only a subset of articles agreed upon within the agreements.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>The dataset is not a sample, but is a filtered version of the full dataset, see Preprocessing/cleaning/labeling for more on this.</p> <p>Who was involved in the data collection process? A team of researchers at the Center for Humanities Computing Aarhus (CHCAA) obtained this dataset using a third party API as well as a manual transfer from some of the parties and would like to thank the dataset owners for  access to their articles.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes articles from 1 December 2019 to  30 April 2021.</p> <p>Were any ethical review processes conducted?</p> <p>No.</p>"},{"location":"datasheets/danews/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>DaNews has been filtered using a series of heuristic filters as well as removing repetitious texts. Following the filtering, DaNews is deduplicated to remove exact and near-duplicates.</p> <p>Of all documents, 9% were filtered based due to low-quality and 4% because they were near-duplicates.</p> <p>For quality filtering, DaNews applies a filter akin to [2] which contains text that:</p> <ul> <li>Contain at least 2 Danish stopwords. For the stopword list we use the one used in SpaCy v.3.1.4.</li> <li>Have a mean word length between 3 and 10.</li> <li>Have a token length between 50 and 100,000.</li> <li>Have less than 5,000,000 characters.</li> <li>Have less than 60% of words containing an alphabetic character.</li> <li>Have a symbol-to-word ratio lower than 10% for hashtags and ellipsis.</li> <li>Have less than 90% of lines starting with a bullet point.</li> <li> <p>have less than 30% of lines ending with an ellipsis.</p> </li> <li> <p>Have a low high degree of repetitious text:</p> </li> <li>Have less than 20% of characters contained within duplicate lines.</li> <li>Have less than 20% of characters contained within duplicate paragraphs.</li> <li>Where the top 2-4 grams constitute less than 20%, 18%, 16%, respectively, of the text. </li> <li>Where the duplicate 5-10 grams constitute less than 25%, 24%, 23%, 22%, 21%, 20% of the text, respectively.</li> </ul> <p>The deduplication removed all documents with a 13-gram Jaccard similarity higher than 80% following the MinHash algorithm [1] using 128 permutations. The MinHash algorithm is a probabilistic data structure for approximating the Jaccard similarity between two sets.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>Yes, the scripts are available here.  the scripts use version 0.0.2 of the dfm package. </p>"},{"location":"datasheets/danews/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train Danish language models. Parts of the dataset have also been used in [3] and [4]</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The scale of the dataset makes it suitable for NLP tasks such as language modeling. Similarly, the structure of the articles makes it a suitable dataset for training text summarisation models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and thus does not evolve over time with the language.  A consequence of this is that it will become increasingly outdated over time.</p> <p>Are there tasks for which the dataset should not be used?</p> <p>This dataset contains Danish articles and thus should not be used for non-Danish language tasks.</p> <p>As the writers of the content are predominantly journalists, it reflects a certain writing style which is unlikely to reflect the Danish language as a whole.</p>"},{"location":"datasheets/danews/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. If you wish access to the dataset you will have to come to an agreement with the individuals Danish newspapers.</p>"},{"location":"datasheets/danews/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up-to-date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/danews/#references","title":"References:","text":"<ul> <li>[1] Broder, Andrei Z. \"On the resemblance and containment of documents.\"         Proceedings. Compression and Complexity of SEQUENCES 1997         (Cat. No. 97TB100171). IEEE, 1997.</li> <li>[2] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,         Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,         T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks,         L. A., Rauh, M., Huang, P.-S., \u2026 Irving, G. (2021).         Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.         https://arxiv.org/abs/2112.11446v2</li> <li>[3] Baglini, R. B., Nielbo, K. L., H\u00e6strup, F., Enevoldsen, K., Vahlstrup, P. B., &amp;          Roepstorff, A. (2021, June 2). When no news is bad news: Detection of negative         events from news media content. https://2021.dhbenelux.org/</li> <li>[4] Nielbo, K. L., Baglini, R. B., Vahlstrup, P. B., Enevoldsen, K. C., Bechmann, A.,         &amp; Roepstorff, A. (2021, January). News information decoupling: An information         signature of catastrophes in legacy news media. https://eadh2020-2021.org/</li> <li>[5] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/danews2.0/","title":"Dataset Card for Danews2.0","text":""},{"location":"datasheets/danews2.0/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 26254962</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/danews2.0/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'id': 'e2a069b9',\n    'text': 'Slottet s\u00e6nker vindebroen 2011-04-12T00:00:00Z NYB...',\n    'source': 'danews2.0',\n    'added': '2024-05-16',\n    'created': '2011-04-12, 2012-04-11',\n    'metadata': {\n        'sub-source': 'LokalAvisen Nyborg',\n        'ArticleUrl': 'https://mediaresearchapi.infomedia.dk/api/v1/article?id=e2a069b9',\n        'Captions': [' Ogs\u00e5 i \u00e5r skal der foretages ark\u00e6ologiske udgravninger ved Nyborg Slot. Her fort\u00e6ller \u00d8stfyns Museers Lars Ewald ( i midten med hat) forrige sommer om fundet af \" den hemmelige tunnel.\" ARKIVFOTO: J\u00d8RGEN HANSEN'],\n        'Authors': [''],\n        'WordCount': 169,\n        'PageIds': ['42'],\n        'Section': {'Name': '', 'Id': '1'}\n    }\n}\n</code></pre></p>"},{"location":"datasheets/danews2.0/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/danews2.0/#license-information","title":"License Information","text":"<p>Agreement with individual newspapers. The agreement is for the Danish Foundation Models project and that models trained on the dataset can be published.</p>"},{"location":"datasheets/daradio/","title":"DaRadio Datasheet","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>License: Not publicly available.</p> <p>DaRadio consists of radio broadcasts from the Danish radio stations DR P1 and Radio24Syv, and contains approximately 140.000 hours of speech. DaRadio includes all shows aired on DR P1 from 2005 to 2021, and all shows aired on Radio24Syv from 2011 to 2019.</p> <p>DaRadio has been deduplicated using a series of heuristics based on metadata. For more on deduplication, see the data cleaning section further below.</p>"},{"location":"datasheets/daradio/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/daradio/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>Data included in DaRadio was collected following the Danish Legal Deposit Act by the Royal Danish Library (RDL). From this, a dataset of Danish speech-only radio was derived by RDL. The dataset was created for research purposes, including training a Danish wav2vec2.0 model. </p> <p>The dataset was preprocessed to remove duplicates by a team of researchers at the Center for Humanities Computing, Aarhus University (CHC) with collaborators from the Danish speech-processing company Alvenir.</p>"},{"location":"datasheets/daradio/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances of the dataset include an mp3 file for each show aired on the two staions within the period. Further metadata include information on date and time of airing, title, short description of the show, and various internal identifiers used by RDL.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>DaRadio consists of a total of 215.582 hours of unprocessed Danish speech radio shows across two stations, DR P1 and Radio24syv. The table below shows the distribution over the stations with and without heuristic rerun removal.</p> Source Duration (hours) Reruns removed P1 145.160 False 97.401 True Radio24syv 70.422 False 44.569 True <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all shows from the two stations in the time period (2005-2021 for DR P1 and 2011-2019 for Radio24syv).</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>The dataset is a subset of all Danish radio. The two stations were chosen for the dataset as they are talk-radio only. </p> <p>Who was involved in the data collection process?</p> <p>The RDL collects Danish radio shows and constructed DaRadio for handing to researchers at CHC.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes radio shows from the period 2005 to 2021.</p> <p>Were any ethical review processes conducted?</p> <p>The RDL collects radio shows in adherence to Danish Archival laws. DaRadio was constructed for a research project, for which a project proposal was accepted by RDL. No other ethical review processes were conducted.</p>"},{"location":"datasheets/daradio/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>DaRadio has been deduplicated using a series of heuristic filters and all files have been converted to 16 Khz .wav files. </p> <p>Reruns/duplicates were identified by the following rules:</p> <ul> <li>If the phrase \"sendt f\u00f8rste gang\" [\"aired the first time\"] or \"genudsendelse\" [\"rerun\"] appeared in the show description.</li> <li>If the title contained \"(G)\" (short for \"genudsendelse\")) </li> <li>If the show was broadcast between 23:00 and 5:00.</li> </ul> <p>The deduplication was coded and conducted by researchers at CHC.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The scripts are available at the following GitHub repository: link.</p>"},{"location":"datasheets/daradio/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish wav2vec2.0 model. </p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 23/10/16 no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>As the dataset only contains un-labelled data, i.e. no transcriptions, it is mainly designed for pre-training language models. However, given the metadata and re-occuring hosts, further processing might make it possible to train e.g. text-to-speech systems. </p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and does not evolve over time with the language, thus will become increasingly outdated over time.</p>"},{"location":"datasheets/daradio/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. An equivalent or updated dataset can be requested at the Royal Danish Library.</p>"},{"location":"datasheets/daradio/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/daradio/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/dbc-abstracts/","title":"dbc-abstracts: all abstract from DBC D1G1TAL","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>dbc-abstracts consists of more than 11.6 million abstracts of books and other materials collected and created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>The dataset contains millions of abstracts in Danish lamnugage, which are supplemented by English, Norwegian, Swedish, German, and other language abstracts.</p>"},{"location":"datasheets/dbc-abstracts/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/dbc-abstracts/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>The dataset was collected and created by DBC D1G1TAL A/S as one of the backbones for their catalogue of books and other materials.</p>"},{"location":"datasheets/dbc-abstracts/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances that comprise this dataset represent short abstracts of books or other materials.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>There are 11,663,988 abstracts in this dataset.</p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all the instances underlying DBC D1G1TAL's database.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>There was no sampling involved.</p> <p>Who was involved in the data collection process?</p> <p>The data was collected by DBC D1G1TAL.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes abstracts created between 1991 and 2024.</p> <p>Were any ethical review processes conducted?</p> <p>No ethical review processes were conducted.</p>"},{"location":"datasheets/dbc-abstracts/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done</p> <p>THe only pre-processing applied is a lossless transformation of the JSONL format to the one preferred by the Danish Foundation Models project, including the addition of timestamps.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The following Python script was used to convert this and the other dbc datasets: <pre><code>import datetime\nimport json\nimport subprocess\nimport tqdm\n\nFILE_NAMES = {\n    \"abstracts.jsonl\": \"abstract\",\n    \"faktalink.jsonl\": None,\n    \"forfatterweb.jsonl\": None,\n    \"reviews.jsonl\": \"content\",\n}\n\nfor file_name, text in tqdm.tqdm(FILE_NAMES.items()):\n    length = int(subprocess.check_output([\"wc\", \"-l\", file_name]).split()[0])\n    with open(f\"dfm-{file_name}\", \"wt\") as f:\n        for line in tqdm.tqdm(open(file_name, \"rt\"), total=length, desc=file_name):\n            d = json.loads(line)\n            if text is None:\n                meta = d[\"metadata\"][\"@graph\"][0]\n                lines = [meta[\"headline\"]]\n                for key, items in d.pop(\"text\").items():\n                    if not key.startswith(\"empty_\"):\n                        lines.append(key)\n                        lines.extend(items)\n                i, t = meta[\"mainEntityOfPage\"].split(\"//\", maxsplit=1)[1], \"\\n\".join(lines)\n            else:\n                i, t = d.pop(\"id\"), d.pop(text)\n            e = {\n                \"id\": i,\n                \"text\": t,\n                \"source\": \"dbc\",\n                \"added\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n                \"created\": \"1991-04-18, 2024-04-18\",\n                \"metadata\": d,\n            }\n            print(json.dumps(e), file=f)\n</code></pre></p>"},{"location":"datasheets/dbc-abstracts/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish encoder-decoder model using a T5 architecture.</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 2024-06-05, no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The dataset contains high-quality texts, many of which are written in Danish. Thus, the dataset could be used for pre-training Danish decocer-only and encoder-only models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>No.</p>"},{"location":"datasheets/dbc-abstracts/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"datasheets/dbc-abstracts/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/dbc-abstracts/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/dbc-faktalink/","title":"dbc-faktalink: all faktalink articles from DBC D1G1TAL","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>dbc-faktalink consists of more than 5 hundred articles created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>All articles are written in Danish lamnugage.</p>"},{"location":"datasheets/dbc-faktalink/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/dbc-faktalink/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>The dataset was collected and created by DBC D1G1TAL A/S for their faktalink.dk website.</p>"},{"location":"datasheets/dbc-faktalink/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances that comprise this dataset represent articles on a variety of topics on aspects relevant to Danish society.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>There are 523 articles in this dataset.</p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all the instances underlying DBC D1G1TAL's faktalink.dk website.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>There was no sampling involved.</p> <p>Who was involved in the data collection process?</p> <p>The data was collected by DBC D1G1TAL.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes articles created between 1991 and 2024.</p> <p>Were any ethical review processes conducted?</p> <p>No ethical review processes were conducted.</p>"},{"location":"datasheets/dbc-faktalink/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done</p> <p>THe only pre-processing applied is a mostly lossless transformation of the JSONL format to the one preferred by the Danish Foundation Models project, including the addition of timestamps.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The following Python script was used to convert this and the other dbc datasets: <pre><code>import datetime\nimport json\nimport subprocess\nimport tqdm\n\nFILE_NAMES = {\n    \"abstracts.jsonl\": \"abstract\",\n    \"faktalink.jsonl\": None,\n    \"forfatterweb.jsonl\": None,\n    \"reviews.jsonl\": \"content\",\n}\n\nfor file_name, text in tqdm.tqdm(FILE_NAMES.items()):\n    length = int(subprocess.check_output([\"wc\", \"-l\", file_name]).split()[0])\n    with open(f\"dfm-{file_name}\", \"wt\") as f:\n        for line in tqdm.tqdm(open(file_name, \"rt\"), total=length, desc=file_name):\n            d = json.loads(line)\n            if text is None:\n                meta = d[\"metadata\"][\"@graph\"][0]\n                lines = [meta[\"headline\"]]\n                for key, items in d.pop(\"text\").items():\n                    if not key.startswith(\"empty_\"):\n                        lines.append(key)\n                        lines.extend(items)\n                i, t = meta[\"mainEntityOfPage\"].split(\"//\", maxsplit=1)[1], \"\\n\".join(lines)\n            else:\n                i, t = d.pop(\"id\"), d.pop(text)\n            e = {\n                \"id\": i,\n                \"text\": t,\n                \"source\": \"dbc\",\n                \"added\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n                \"created\": \"1991-04-18, 2024-04-18\",\n                \"metadata\": d,\n            }\n            print(json.dumps(e), file=f)\n</code></pre></p>"},{"location":"datasheets/dbc-faktalink/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish encoder-decoder model using a T5 architecture.</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 2024-06-05, no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The dataset contains high-quality texts, all of which are written in Danish. Thus, the dataset could be used for pre-training Danish decocer-only and encoder-only models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>The title, headings, and paragraphs have been concatenated into one text separated by new lines. The semantic information of what constitutes a heading or a paragraph is thus lost. This should be easy enough to recover in most cases but might be hard to do in cases where the heading ends in a punctuation sign.</p>"},{"location":"datasheets/dbc-faktalink/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"datasheets/dbc-faktalink/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/dbc-faktalink/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/dbc-forfatterweb/","title":"dbc-forfatterweb: all forfatterweb articles from DBC D1G1TAL","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>dbc-forfatterweb consists of more than 1 thousand articles created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>All articles are written in Danish lamnugage.</p>"},{"location":"datasheets/dbc-forfatterweb/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/dbc-forfatterweb/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>The dataset was collected and created by DBC D1G1TAL A/S for their forfatterweb.dk website.</p>"},{"location":"datasheets/dbc-forfatterweb/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances that comprise this dataset represent articles on Danish writers.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>There are 1,189 articles in this dataset.</p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all the instances underlying DBC D1G1TAL's forfatterweb.dk website.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>There was no sampling involved.</p> <p>Who was involved in the data collection process?</p> <p>The data was collected by DBC D1G1TAL.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes articles created between 1991 and 2024.</p> <p>Were any ethical review processes conducted?</p> <p>No ethical review processes were conducted.</p>"},{"location":"datasheets/dbc-forfatterweb/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done</p> <p>THe only pre-processing applied is a mostly lossless transformation of the JSONL format to the one preferred by the Danish Foundation Models project, including the addition of timestamps.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The following Python script was used to convert this and the other dbc datasets: <pre><code>import datetime\nimport json\nimport subprocess\nimport tqdm\n\nFILE_NAMES = {\n    \"abstracts.jsonl\": \"abstract\",\n    \"faktalink.jsonl\": None,\n    \"forfatterweb.jsonl\": None,\n    \"reviews.jsonl\": \"content\",\n}\n\nfor file_name, text in tqdm.tqdm(FILE_NAMES.items()):\n    length = int(subprocess.check_output([\"wc\", \"-l\", file_name]).split()[0])\n    with open(f\"dfm-{file_name}\", \"wt\") as f:\n        for line in tqdm.tqdm(open(file_name, \"rt\"), total=length, desc=file_name):\n            d = json.loads(line)\n            if text is None:\n                meta = d[\"metadata\"][\"@graph\"][0]\n                lines = [meta[\"headline\"]]\n                for key, items in d.pop(\"text\").items():\n                    if not key.startswith(\"empty_\"):\n                        lines.append(key)\n                        lines.extend(items)\n                i, t = meta[\"mainEntityOfPage\"].split(\"//\", maxsplit=1)[1], \"\\n\".join(lines)\n            else:\n                i, t = d.pop(\"id\"), d.pop(text)\n            e = {\n                \"id\": i,\n                \"text\": t,\n                \"source\": \"dbc\",\n                \"added\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n                \"created\": \"1991-04-18, 2024-04-18\",\n                \"metadata\": d,\n            }\n            print(json.dumps(e), file=f)\n</code></pre></p>"},{"location":"datasheets/dbc-forfatterweb/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish encoder-decoder model using a T5 architecture.</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 2024-06-05, no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The dataset contains high-quality texts, all of which are written in Danish. Thus, the dataset could be used for pre-training Danish decocer-only and encoder-only models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>The title, headings, and paragraphs have been concatenated into one text separated by new lines. The semantic information of what constitutes a heading or a paragraph is thus lost. This should be easy enough to recover in most cases but might be hard to do in cases where the heading ends in a punctuation sign.</p>"},{"location":"datasheets/dbc-forfatterweb/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"datasheets/dbc-forfatterweb/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/dbc-forfatterweb/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/dbc-reviews/","title":"dbc-reviews: all reviews from DBC D1G1TAL","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>dbc-reviews consists of more than 214 thousand reviews of books and other materials collected and created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>The dataset contains thousands of reviews in Danish lamnugage, which are supplemented by English, Norwegian, Swedish, German, and other language reviews.</p>"},{"location":"datasheets/dbc-reviews/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [1], we add the following datasheet. </p>"},{"location":"datasheets/dbc-reviews/#motivation","title":"Motivation:","text":"<p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p> <p>The dataset was collected and created by DBC D1G1TAL A/S as one of the backbones for their catalogue of books and other materials.</p>"},{"location":"datasheets/dbc-reviews/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>Instances that comprise this dataset represent reviews of books or other materials.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>There are 214,035 reviews in this dataset.</p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>The dataset contains all the instances underlying DBC D1G1TAL's database.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>There was no sampling involved.</p> <p>Who was involved in the data collection process?</p> <p>The data was collected by DBC D1G1TAL.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes reviews created between 1991 and 2024.</p> <p>Were any ethical review processes conducted?</p> <p>No ethical review processes were conducted.</p>"},{"location":"datasheets/dbc-reviews/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done</p> <p>THe only pre-processing applied is a lossless transformation of the JSONL format to the one preferred by the Danish Foundation Models project, including the addition of timestamps.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>The following Python script was used to convert this and the other dbc datasets: <pre><code>import datetime\nimport json\nimport subprocess\nimport tqdm\n\nFILE_NAMES = {\n    \"abstracts.jsonl\": \"abstract\",\n    \"faktalink.jsonl\": None,\n    \"forfatterweb.jsonl\": None,\n    \"reviews.jsonl\": \"content\",\n}\n\nfor file_name, text in tqdm.tqdm(FILE_NAMES.items()):\n    length = int(subprocess.check_output([\"wc\", \"-l\", file_name]).split()[0])\n    with open(f\"dfm-{file_name}\", \"wt\") as f:\n        for line in tqdm.tqdm(open(file_name, \"rt\"), total=length, desc=file_name):\n            d = json.loads(line)\n            if text is None:\n                meta = d[\"metadata\"][\"@graph\"][0]\n                lines = [meta[\"headline\"]]\n                for key, items in d.pop(\"text\").items():\n                    if not key.startswith(\"empty_\"):\n                        lines.append(key)\n                        lines.extend(items)\n                i, t = meta[\"mainEntityOfPage\"].split(\"//\", maxsplit=1)[1], \"\\n\".join(lines)\n            else:\n                i, t = d.pop(\"id\"), d.pop(text)\n            e = {\n                \"id\": i,\n                \"text\": t,\n                \"source\": \"dbc\",\n                \"added\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),\n                \"created\": \"1991-04-18, 2024-04-18\",\n                \"metadata\": d,\n            }\n            print(json.dumps(e), file=f)\n</code></pre></p>"},{"location":"datasheets/dbc-reviews/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train a Danish encoder-decoder model using a T5 architecture.</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>No, but as of 2024-06-05, no others have used the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The dataset contains high-quality texts, many of which are written in Danish. Thus, the dataset could be used for pre-training Danish decocer-only and encoder-only models.</p> <p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p> <p>No</p>"},{"location":"datasheets/dbc-reviews/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"datasheets/dbc-reviews/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/dbc-reviews/#references","title":"References:","text":"<ul> <li>[1] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,         and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> </ul>"},{"location":"datasheets/hopetwitter/","title":"HopeTwitter","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>HopeTwitter consists of tweets collected from the Twitter API using a stopword list and consists of 32.5 million tweets across 538,398 unique users. HopeTwitter includes tweets from 2019-01-01 to 2021-04-30.</p> <p>HopeTwitter, have been filtered to only include Danish tweets, based on language tag from Twitter API. Similarly, HopeTwitter have had low-quality tweets have removed and then deduplicated to remove exact and near-duplicates. For more on data cleaning see section; \"Preprocessing/cleaning/labeling\".</p> <p>HopeTwitter includes a total of 0.97 billion tokens before filtering and includes 0.48 billion (50%) after.</p>"},{"location":"datasheets/hopetwitter/#datasheet","title":"Datasheet","text":"<p>Following the recommendation and framework of [3] we add the following datasheet. </p>"},{"location":"datasheets/hopetwitter/#motivation","title":"Motivation","text":"<p>**For what purpose was the dataset created? Who created the dataset? Who funded the  creation of the dataset? **</p> <p>HopeTwitter was initially collected as a part of the HOPE project, examining societal behaviour during the COVID-19 pandemic. Next, HopeTwitter was cleaned in preparation for pre-training Danish language models by a team of researchers at Center for Humanities Computing Aarhus  (CHCAA), using a codebase jointly developed with partners from academia and industry, including KMD, Ekstra Bladet, Bristol University and Deepdivr. For more on collaborators on this project see the GitHub repository.</p> <p>Any other comments?</p> <p>No.</p>"},{"location":"datasheets/hopetwitter/#composition","title":"Composition","text":"<p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p> <p>HopeTwitter consists of tweets containing at least one of a series of stopwords, collected through the Twitter API. See \"If the dataset is a sample from a larger set, what was the sampling strategy?\" for the stopword list.</p> <p>How many instances are there in total (of each type, if appropriate)?</p> <p>The dataset consist of 32,499,019 documents where 14,399,284 (44%) were considered duplicates. </p> <p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p> <p>No. It does not contain all instances of Danish Twitter as there are likely some Danish tweets which does not include a stopword.</p> <p>Is there a label or target associated with each instance? If so, please provide a description.</p> <p>No.</p> <p>Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p> <p>No splits are performed on this dataset.</p> <p>If the dataset is a sample from a larger set, what was the sampling strategy?</p> <p>Tweets are streamed continuously using queried a set of the highest  frequency Scandinavian-specific keywords from Danish, Norwegian (Bokm\u00e5l) and Swedish, resulting in the following list: <pre><code>aften, aldrig, alltid, altid, andet, arbejde, bedste, beh\u00f6ver, beh\u00f8ver, beklager,\nber\u00e4tta, betyr, blev, blevet, blir, blitt, blive, bliver, bruge, burde, b\u00e4ttre, b\u00e5e\nb\u00f8r, deim, deires, ditt, drar, drepe, dykk, dykkar, d\u00e4r, d\u00f6d, d\u00f6da, d\u00f8d, d\u00f8de, efter,\nelsker, endnu, faen, fandt, feil, fikk, finner, flere, forst\u00e5r, fortelle, fortfarande,\nfortsatt, fort\u00e6lle, fr\u00e5n, f\u00e5, f\u00e5et, f\u00e5r, f\u00e5tt, f\u00f6rl\u00e5t, f\u00f6rsta, f\u00f6rs\u00f6ker, f\u00f8r, f\u00f8rst,\nf\u00f8rste, gick, gikk, gillar, gjennom, gjerne, gjorde, gjort, gj\u00f8r, gj\u00f8re, godt, g\u00e5, g\u00e5ng,\ng\u00e5r, g\u00f6ra, g\u00f8r, g\u00f8re, hadde, hall\u00e5, havde, hedder, helt, helvete, hende, hendes, hennes,\nherregud, hjelp, hjelpe, hjem, hj\u00e4lp, hj\u00e5, hj\u00e6lp, hj\u00e6lpe, honom, hossen, hvem, hvis,\nhvordan, hvorfor, h\u00e4nder, h\u00e4r, h\u00e5ll, h\u00e5ller, h\u00f8r, h\u00f8re, h\u00f8rer, igjen, ikkje, ingenting,\ninkje, inte, intet, jeres, j\u00e4vla, kanske, kanskje, kender, kjenner, korleis, kvarhelst,\nkveld, kven, kvifor, k\u00e4nner, ledsen, lenger, lidt, livet, l\u00e4ngre, l\u00e5t, l\u00e5ter, l\u00e6nge,\nmeget, menar, mycket, mykje, m\u00e5, m\u00e5de, m\u00e5nga, m\u00e5r, m\u00e5ske, m\u00e5ste, m\u00e5tte, navn, nogen,\nnoget, nogle, noko, nokon, nokor, nokre, n\u00e5gon, n\u00e5got, n\u00e5gra, n\u00e5n, n\u00e5r, n\u00e5t, n\u00f8dt,\nocks\u00e5, ogs\u00e5, pengar, penger, pratar, pr\u00f8ver, p\u00e5, redan, rundt, r\u00e4tt, sagde, saker,\nsamma, sammen, selv, selvf\u00f8lgelig, sidan, sidste, siger, sikker, sikkert, sj\u00e4lv, skete,\nskjedde, skjer, skulle, sluta, slutt, snakke, snakker, snill, sn\u00e4lla, somt, stadig,\nstanna, sted, st\u00e5r, synes, s\u00e4ger, s\u00e4tt, s\u00e5, s\u00e5dan, s\u00e5g, s\u00e5nn, tager, tiden, tilbage,\ntilbake, tillbaka, titta, trenger, trodde, troede, tror, tv\u00e5, tycker, t\u00e4nker, uden,\nundskyld, unnskyld, urs\u00e4kta, uten, varf\u00f6r, varit, varte, veldig, venner, verkligen,\nvidste, vilken, virkelig, visste, v\u00e4g, v\u00e4l, v\u00e4ldigt, v\u00e4n, v\u00e5r, v\u00e5ra, v\u00e5re, v\u00e6k, v\u00e6r, \nv\u00e6re, v\u00e6ret, \u00e4lskar, \u00e5h, \u00e5r, \u00e5t, \u00f6ver\n</code></pre></p> <p>Who was involved in the data collection process?</p> <p>A team of researchers at the Center for Humanities Computing Aarhus (CHCAA), including Kristoffer Nielbo and Peter Bjerregaard Vahlstrup, in collaboration with Rebekah Baglini, at the School of Communcation and Culture at Aarhus university.</p> <p>Over what timeframe was the data collected?</p> <p>The dataset includes tweets from the period 2019-01-01 to 2021-04-30.</p> <p>Were any ethical review processes conducted?</p> <p>No</p>"},{"location":"datasheets/hopetwitter/#preprocessingcleaninglabeling","title":"Preprocessing/cleaning/labeling","text":"<p>Was any preprocessing/Cleaning/Labeling of the data done  (e.g., discretization or bucketing, tokenization, part-of-speech tagging,  SIFT feature extraction, removal of instances, processing of missing values)?</p> <p>Firstly, HopeTwitter had non-Danish tweets removed, after which a series of heuristic filters were applied, including the removal of repetitious texts. Following the filtering, HopeTwitter was deduplicated, removing both exact duplicates and near-duplicates.</p> <p>Of all documents, 3,023,427 (9%) were filtered due to low-quality and 14,399,284 (33%) because they were near-duplicates.</p> <p>For the quality filtering, HopeTwitter applies a filter akin to [2] which contains text that:</p> <ul> <li>Contain at least 2 Danish stopwords. For the stopword list we use the one used in SpaCy v.3.1.4.</li> <li>Have a mean word length between 2 and 14.</li> <li>Have a token length between 10 and 100,000.</li> <li>Have less than 5,000,000 characters.</li> <li> <p>Have less than 60% of words containing an alphabetic character.</p> </li> <li> <p>Have low high degree of repetitious text:</p> </li> <li>Have less than 20% of characters contained within duplicate lines.</li> <li>Have less than 20% of characters contained within duplicate paragraphs.</li> <li>Where the top 2-4 grams constitute less than 20%, 18%, 16%, respectively, of the text. </li> <li>Where the duplicate 5-10 grams constitute less than 25%, 24%, 23%, 22%, 21%, 20% of the text, respectively.</li> </ul> <p>The deduplication removed all documents with a 10-gram Jaccard similarity higher than 80% following the MinHash algorithm [1] using 128 permutations. The MinHash algorithm is a probabilistic data structure for approximating the Jaccard similarity between two sets.</p> <p>Is the software used to preprocess/clean/label the instances available?</p> <p>Yes, the scripts are available here. The scripts use version 0.0.2 of the dfm package. </p>"},{"location":"datasheets/hopetwitter/#uses","title":"Uses","text":"<p>Has the dataset been used for any tasks already?</p> <p>Yes, the dataset has been used to pre-train Danish language models. Parts of the dataset have also been used in HOPE project reports  and in [4].</p> <p>Is there a repository that links to any or all papers or systems that use the dataset?</p> <p>There is a website for the HOPE project for which the dataset was initially collected. This website contains report and articles regarding the dataset.</p> <p>What (other) tasks could the dataset be used for?</p> <p>The scale of the dataset makes it suitable for NLP tasks such as language modelling. Similarly, one could imagine using the conversation structure could be used to train conversational chatbots.</p> <p>Is there anything about the composition of the dataset or the way it was collected and  preprocessed/cleaned/labeled that might impact future uses?</p> <p>This dataset is static and thus does not evolve over time with the language.  A consequence of this is that it will become increasingly outdated over time. However, it possible to extend the dataset by a continual collection of tweets.</p> <p>Are there tasks for which the dataset should not be used?</p> <p>HopeTwitter contains Danish tweets and thus should not be used for non-Danish language tasks.</p> <p>As the writers of the content is predominantly journalists, politicians, influencers, and academics, it reflects a certain social group which is unlikely to reflect Danish population as a whole.</p>"},{"location":"datasheets/hopetwitter/#distribution","title":"Distribution","text":"<p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p> <p>Data will only be available at the entity during the project. After the project the data will be archived for a period of five years to comply with the university [policy] for research integrity. After the five years, the data will be registered at the national archives as required by executive order 514 for potential long-term deposit.</p>"},{"location":"datasheets/hopetwitter/#citation","title":"Citation","text":"<p>If you wish to cite this work please see our GitHub page for an up to date citation: https://github.com/centre-for-humanities-computing/danish-foundation-models</p>"},{"location":"datasheets/hopetwitter/#references","title":"References:","text":"<ul> <li>[1] Broder, Andrei Z. \"On the resemblance and containment of documents.\"     Proceedings. Compression and Complexity of SEQUENCES 1997     (Cat. No. 97TB100171). IEEE, 1997.</li> <li>[2] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,      Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,      T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks,      L. A., Rauh, M., Huang, P.-S., \u2026 Irving, G. (2021).     Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.     https://arxiv.org/abs/2112.11446v2</li> <li>[3] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daum\u00e9 III,     and K. Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.</li> <li>[4]\u00a0Johansen, N., Marjanovic, S. V., Kjaer, C. V., Baglini, R. B., &amp; Adler-Nissen, R.      (2022). Ridiculing the \u201ctinfoil hats:\u201d Citizen responses to COVID-19 misinformation     in the Danish facemask debate on Twitter. Harvard Kennedy School Misinformation     Review. https://doi.org/10.37016/mr-2020-93</li> </ul>"},{"location":"datasheets/memo/","title":"Dataset Card for memo","text":""},{"location":"datasheets/memo/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 858</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/memo/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'id': '130024227090',\n    'text': 'I . \\nI den fornemste gade l\u00e5 en pr\u00e6gtig , \\ngammeld...',\n    'source': 'memo',\n    'added': '2024-05-16',\n    'created': '1870-01-01, 1970-01-01',\n    'metadata': {\n        'file_received': 'y',\n        'filename': '1870_AndersenHC_LykkePeer.pdf',\n        'firstname': 'H.C.',\n        'surname': 'Andersen',\n        'pseudonym': None,\n        'gender': 'm',\n        'nationality': 'dk',\n        'title': 'Lykke-Peer',\n        'subtitle': None,\n        'volume': None,\n        'year': 1870,\n        'pages': '183',\n        'illustrations': 'n',\n        'typeface': 'gothic',\n        'publisher': 'Reitzel',\n        'price': '2,25',\n        'file_status': 'Modtaget fra KB 7.4.2022 DEL2 sending 5',\n        'notes': \"OBS! PDF'en er ren tekst i antikva, men den fysiske bog formentlig fraktur. Det kalder p\u00e5 separate kolonner: pdf-typeface eller file-typeface og book-typeface. /PD\",\n        'filepath': None,\n        'fileformat': 'pdftxt',\n        'txt_received': 'y',\n        'readable': 'y',\n        'historical': 'n',\n        'period': None,\n        'period_notes': None,\n        'novel_start': '10',\n        'novel_end': '190',\n        'novelstart_rescan': None,\n        'novelend_rescan': None,\n        'start_end_page_notes': None,\n        'serialno': 12.0,\n        'quarantine': None,\n        'discard': None,\n        'sub-source': 'KB'\n    }\n}\n</code></pre></p>"},{"location":"datasheets/memo/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/memo/#license-information","title":"License Information","text":"Creative Commons Attribution 4.0 International <p> Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.  https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/cc-by-4.0.md </p>"},{"location":"datasheets/netarkivet_text/","title":"NAT: Netarkivet Text","text":"<p>Version: 1.0.0</p> <p>Homepage: https://github.com/centre-for-humanities-computing/danish-foundation-models</p> <p>license: Not publicly available.</p> <p>This datasheet is currently being revised \ud83d\udee0\ufe0f</p>"},{"location":"datasheets/nordjylland_news/","title":"Dataset Card for nordjylland_news","text":""},{"location":"datasheets/nordjylland_news/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 75219</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/nordjylland_news/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    \"id\": \"nordjylland-news0\",\n    \"text\": \"Opdatering: Manden er nu fundet af Nordjyllands Po...\",\n    \"source\": \"nordjylland_news\",\n    \"added\": \"2024-05-23\",\n    \"created\": \"2024-05-23, 2025-05-23\",\n    \"metadata\": {\n        \"summary\": \"Nye oplysninger i sagen om en forsvunden mand har endnu en gang f\\u00e5et politiet til at henvende sig til borgerne.\",\n        \"text_len\": 1739,\n        \"summary_len\": 111,\n        \"sub-source\": \"TV2 Nord\"\n    }\n}\n</code></pre></p>"},{"location":"datasheets/nordjylland_news/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/nordjylland_news/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/scrape_hovedstaden/","title":"Dataset Card for scape_hovedstaden","text":""},{"location":"datasheets/scrape_hovedstaden/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 24752</li> <li>Languages: Danish</li> </ul>"},{"location":"datasheets/scrape_hovedstaden/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'id': 'doc_hovedstaden_Rigshospitalet_Bed\u042bvelse og Intensiv Behandling (NEU)_Transkraniel Doppler - NIA 6021',\n    'text': 'Transkraniel Doppler - NIA 6021\\n\\nM\u00e5lgrupper og anv...',\n    'source': 'scrape_hovedstaden',\n    'added': '2024-05-23',\n    'created': '2023-11-16, 2024-04-04',\n    'metadata': {\n        'subject': 'health',\n        'language': 'danish',\n        'organization': 'The Danish Agency for Digitalisation', 'source-pretty': 'University of Southern Denmark (SDU) &amp; Capital Region',\n        'URL': 'https://sprogteknologi.dk/dataset/1076892a-14ee-4f14-a9db-32efb03c40c9'\n    }\n}\n</code></pre></p>"},{"location":"datasheets/scrape_hovedstaden/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/scrape_hovedstaden/#license-information","title":"License Information","text":"Creative Commons Zero v1.0 Universal <p> Creative Commons Legal Code  CC0 1.0 Universal </p>"},{"location":"datasheets/swedish_gigaword/","title":"Dataset Card for swedish_gigaword","text":""},{"location":"datasheets/swedish_gigaword/#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of records: 50185534</li> <li>Languages: Swedish</li> </ul>"},{"location":"datasheets/swedish_gigaword/#dataset-sturcture","title":"Dataset Sturcture","text":"<p>An example from the dataset looks as follows. <pre><code>{\n    'id': '1950:10',\n    'text': 'National Librar of Sweden Denna bok digitaliserade...', 'source': 'swedish_gigaword',\n    'added': '2024-05-23',\n    'created': '1950-01-01, 1950-12-31',\n    'metadata': {\n        'year': '1950',\n        'genre': 'government',\n        'librisid': '13537973'\n    }\n}\n</code></pre></p>"},{"location":"datasheets/swedish_gigaword/#data-fields","title":"Data Fields","text":"<ul> <li>id: source-specific identifier.</li> <li>text: textual content of the document.</li> <li>source: source of the data.</li> <li>added: timestamp ai2 acquired this data.</li> <li>created\": timestamp when original document was created (best-guess if not available)</li> <li>metadata: source-specific metadata.</li> </ul>"},{"location":"datasheets/swedish_gigaword/#license-information","title":"License Information","text":"Creative Commons Attribution 4.0 International <p> Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.  https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/cc-by-4.0.md </p>"},{"location":"taggers_stats/taggers_speed/","title":"Statistics on Dolma and Scandi taggers","text":"<p>The dataset for testing is a subset of DAGW, which is only consist of documents from domain <code>Wiki &amp; Books</code> (430837 in total). However, during testing, 161915 documents that only contains white space(s) were found out due to the <code>ZeroDivisionErro</code> in tagger <code>pii_regex_v1</code> when there is no empty documents.</p> <p>Finally, the test on taggers performed on cleaned <code>Wiki &amp; Books</code> section of DAGW (268922 in total), each following tagger was tested only once using one proccess.</p> <p>Here is an example of how to use a tagger (Detailed documentation):</p> <p><pre><code>dolma tag \\\n    --documents \"/work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz\" \\\n    --experiment char_length_v1 \\\n    --taggers char_length_v1 \\\n    --processes 1\n</code></pre> Remark: <code>pii_presidio_v1</code> has a maximum text of length 1000000:  <pre><code>dolma.core.errors.DolmaFatalError: Failed to process /work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz due to ValueError: [E088] Text of length 1215638 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre></p> # Dolma Tagger Description Process Time (In total, Speed) 1 char_length_v1 Computes document length in characters 16s, 16.2kd/s 2 char_length_with_paragraphs_v1 Computes document and paragraph length in characters 49s, 5.40kd/s 3 cld2_en_doc_v2 Detects document language using cld2 56s, 4.76kd/s 4 olmo_pretokenizer_v1 Counts number of tokens using OLMo v1 pre-tokenizer 6m57s, 645d/s 5 olmo_pretokenizer_with_paragraphs_v1 Counts tokens in document and paragraphs using OLMo v1 pre-tokenizer 7m02s, 636d/s 6 whitespace_tokenizer_v1 Counts whitespace-separated tokens in document 1m00s, 4.47kd/s 7 whitespace_tokenizer_with_paragraphs_v1 Counts whitespace-separated tokens in document and paragraphs 1m39s, 2.70kd/s 8 random_number_v1 Assigns a random number to each document 17s, 15.6kd/s 9 ft_lang_id_en_doc_v2 Uses fastText to detect the language of the document 2m28s, 1.82kd/s 10 ft_lang_id_en_paragraph_v2 Uses fastText to detect the language of each paragraph 6m21s, 705d/s 11 ft_lang_id_en_paragraph_with_doc_score_v2 Uses fastText to detect the language of each paragraph and assigns a score based on the fraction of English paragraphs 6m16s, 715d/s 12 gopher_v1 Tags spans of documents matching\u00a0Deepmind's Gopher\u00a0removal rules 15m49s, 283d/s 13 c4_v1 Implements taggers used to generate the\u00a0C4\u00a0dataset 3m50s, 1.17kd/s 14 c4_v2 Faster implementation of the C4 taggers 2m08s, 2.10kd/s 15 pii_presidio_v1 Tags spans of documents that contain personally identifiable information (PII) using the\u00a0Presidio Analyzer\u00a0library way to slow: about 7s per document. However <code>analyzer_results</code> in pii.py defines the language as English if . See line 110 in here 16 pii_regex_v1 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions 2m55s, 1.53kd/s 17 pii_regex_v2 Faster implementation of\u00a0<code>pii_regex_v1</code> 2m51s, 1.57kd/s 18 pii_regex_with_counts_v2 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions. It also counts the number of matches for each regular expression 2m43s, 1.65kd/s 19 pii_regex_with_counts_fast_v2 Faster implementation of\u00a0<code>pii_regex_with_counts_v2</code> 1m01s, 4.36kd/s 20 cld2_scandi_doc Language Detection using cld2 1m11s, 3.79kd/s 21 cld2_scandi_paragraph Language Detection on paragraph level using cld2 5m59s, 748d/s 22 ft_lang_id_scandi_doc FastText Language Detection 3m14s, 1.38kd/s 23 ft_lang_id_scandi_paragraph FastText Language Detection on paragraph level 14m06s, 318d/s 24 cld2_scandi_paragraph_with_doc_score Language Detection on paragraph level with a total score using cld2 8m04s, 556d/s 25 ft_lang_id_scandi_paragraph_with_doc_score FastText Language Detection on paragraph level with a total score 14m37s, 306d/s 26 jigsaw_hatespeech_document_v2 Tags documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 1m38s, 2.74kd/s 27 jigsaw_hatespeech_sentence_v2 Tags spans of documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 9m45s, 460d/s 28 jigsaw_nsfw_document_v1 Tags documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 6m40s, 671d/s 29 jigsaw_nsfw_sentence_v2 Tags spans of documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 9m02s, 496d/s"},{"location":"tutorials/finetune/","title":"Efficiently Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</pre> # Uncomment to install packages (already done for you) # %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121 # %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\" In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import getpass  # For loading the finetuning datasets from datasets import load_dataset  # For loading and finetuning the models from unsloth import FastLanguageModel from trl import SFTTrainer, setup_chat_format from transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None In\u00a0[\u00a0]: Copied! <pre>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</pre> RANDOM_SEED = 42  MODEL_CONFIGURATION = dict(     model_name=\"danish-foundation-models/munin-7b-alpha\",     max_seq_length=2048,       dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs     load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.     attn_implementation=\"flash_attention_2\" )  PEFT_CONFIGURATION = dict(     r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128     target_modules=[         \"q_proj\",          \"k_proj\",          \"v_proj\",          \"o_proj\",          \"gate_proj\",          \"up_proj\",          \"down_proj\",     ],     lora_alpha = 16,     lora_dropout = 0,  # Supports any, but = 0 is optimized     bias = \"none\",  # Supports any, but = \"none\" is optimized     use_gradient_checkpointing = True,     use_rslora = False,  # Support rank stabilized LoRA     loftq_config = None,  # And LoftQ     random_state = RANDOM_SEED, )  FINETUNING_CONFIGURATION = dict(     per_device_train_batch_size=8,     gradient_accumulation_steps=1,     warmup_steps=5,     num_train_epochs=1,     learning_rate=2e-4,     weight_decay=0.01,     lr_scheduler_type=\"linear\", ) In\u00a0[\u00a0]: Copied! <pre>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</pre> model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN) model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer) model = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION) <p>Load the dataset from Hugging Face Hub:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</pre> dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\") print(f\"Number of samples in dataset: {len(dataset):,}\") <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> In\u00a0[\u00a0]: Copied! <pre>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</pre> n_samples = 1000 dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples)) <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> In\u00a0[\u00a0]: Copied! <pre>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</pre> def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:     \"\"\"This converts the sample to the standardised ChatML format.      Args:         sample:             The data sample.      Returns:         The sample set up in the ChatML format.     \"\"\"     return {         \"messages\": [             {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},             {\"role\": \"user\", \"content\": sample[\"question\"]},             {\"role\": \"assistant\", \"content\": sample[\"response\"]}         ]     }  dataset = dataset.map(create_conversation, batched=False) In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</pre> trainer = SFTTrainer(     model=model,     tokenizer=tokenizer,     train_dataset=dataset,     max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],     dataset_num_proc=4,     packing=True,  # Can make training 5x faster for short sequences.     args = TrainingArguments(         optim=\"adamw_8bit\",         fp16=not torch.cuda.is_bf16_supported(),         bf16=torch.cuda.is_bf16_supported(),         logging_steps=3,         seed=RANDOM_SEED,         output_dir=\"outputs\",         **FINETUNING_CONFIGURATION     ), ) In\u00a0[\u00a0]: Copied! <pre># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</pre> # Log some GPU stats before we start the finetuning gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(     f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"     f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\" ) In\u00a0[\u00a0]: Copied! <pre># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</pre> # This is where the actual finetuning is happening trainer_stats = trainer.train() In\u00a0[\u00a0]: Copied! <pre># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</pre> # Log some post-training GPU statistics used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory / max_memory * 100, 3) lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) print(     f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"     f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"     \"was used for LoRa.\" ) <p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id,     use_cache=False,  #\u00a0Required by unsloth ) <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too     ), ]  outputs = model.generate(     input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),     streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN) <p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> In\u00a0[\u00a0]: Copied! <pre># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</pre> # Merge to 16bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)  # Merge to 4bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN) <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> In\u00a0[\u00a0]: Copied! <pre># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</pre> # Just LoRA adapters # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN) <p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> In\u00a0[\u00a0]: Copied! <pre># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</pre> # Save to 8bit Q8_0 # model.save_pretrained_gguf(\"model\", tokenizer) # model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)  # Save to 16bit GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)  # Save to q4_k_m GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN) <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"tutorials/finetune/#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/finetune/#efficiently-finetuning-language-models","title":"Efficiently Finetuning Language Models\u00b6","text":""},{"location":"tutorials/finetune/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/finetune/#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/finetune/#configure-the-model","title":"Configure the Model\u00b6","text":""},{"location":"tutorials/finetune/#load-the-model","title":"Load the Model\u00b6","text":""},{"location":"tutorials/finetune/#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":""},{"location":"tutorials/finetune/#finetune","title":"Finetune!\u00b6","text":""},{"location":"tutorials/finetune/#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/finetune/#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/finetune/#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks\u00b6","text":""},{"location":"tutorials/finetune/#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM\u00b6","text":""},{"location":"tutorials/finetune/#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion\u00b6","text":""},{"location":"tutorials/finetune_qlora/","title":"Finetuning Language Models using QLoRA","text":"<p>We demonstrate how to finetune a <code>munin-7b-alpha</code> or another large language model (LLM) on a Danish translated instruction tuning dataset, with LoRA and tools from the PyTorch and Hugging Face ecosystem. This notebook can be run on on a typical consumer GPU (e.g. NVIDIA T4 16GB).</p> <p>This notebook takes some liberties to ensure simplicity and readability, while remaining reasonably efficient. However if you want a more efficient approach, see the tutorial on (efficiently) finetuning language models.</p> In\u00a0[1]: Copied! <pre>%pip install -q datasets bitsandbytes peft trl accelerate sentencepiece protobuf --upgrade\n\n# Description of the libraries:\n# - Datasets: A high-performant dataset library for easily sharing and accessing datasets from the huggingface Hub at huggingface.co/datasets\n# - bitsandbytes: A lightweight library for loading models using low-precession (this makes it faster and use less memory)\n# - Transformers: A high-level library for working with language LLMs\n# - PEFT: A library for parameter-efficient fine-tuning of LLMs\n# - TRL: A library for training LLMs using reinforcement learning\n# - Accelerate: A library for distributed and efficient training of LLMs\n# - Sentencepiece: A library for tokenizing text required by some models\n</pre> %pip install -q datasets bitsandbytes peft trl accelerate sentencepiece protobuf --upgrade  # Description of the libraries: # - Datasets: A high-performant dataset library for easily sharing and accessing datasets from the huggingface Hub at huggingface.co/datasets # - bitsandbytes: A lightweight library for loading models using low-precession (this makes it faster and use less memory) # - Transformers: A high-level library for working with language LLMs # - PEFT: A library for parameter-efficient fine-tuning of LLMs # - TRL: A library for training LLMs using reinforcement learning # - Accelerate: A library for distributed and efficient training of LLMs # - Sentencepiece: A library for tokenizing text required by some models In\u00a0[2]: Copied! <pre># print the version of the libraries for reproducibility\nimport datasets\nimport bitsandbytes\nimport transformers\nimport peft\nimport trl\nimport accelerate\nimport sentencepiece\n\nprint(f\"datasets: {datasets.__version__}\")\nprint(f\"bitsandbytes: {bitsandbytes.__version__}\")\nprint(f\"transformers: {transformers.__version__}\")\nprint(f\"peft: {peft.__version__}\")\nprint(f\"trl: {trl.__version__}\")\nprint(f\"accelerate: {accelerate.__version__}\")\nprint(f\"sentencepiece: {sentencepiece.__version__}\")\n</pre> # print the version of the libraries for reproducibility import datasets import bitsandbytes import transformers import peft import trl import accelerate import sentencepiece  print(f\"datasets: {datasets.__version__}\") print(f\"bitsandbytes: {bitsandbytes.__version__}\") print(f\"transformers: {transformers.__version__}\") print(f\"peft: {peft.__version__}\") print(f\"trl: {trl.__version__}\") print(f\"accelerate: {accelerate.__version__}\") print(f\"sentencepiece: {sentencepiece.__version__}\")  <pre>datasets: 2.19.1\nbitsandbytes: 0.43.1\ntransformers: 4.40.1\npeft: 0.10.0\ntrl: 0.8.6\naccelerate: 0.30.0\nsentencepiece: 0.2.0\n</pre> In\u00a0[3]: Copied! <pre>from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_name = \"mhenrichsen/danskgpt-tiny-chat\" # download a smaller model (due to memory constraint of Colab)\n# model_name=\"danish-foundation-models/munin-7b-alpha\" # if you have more memory you can use this\n\n\n# Load base model\n# - optionally load the model in 4-bit precision (recommended for large models to save memory)\nbnb_config = BitsAndBytesConfig(\n     load_in_4bit=True,\n     bnb_4bit_use_double_quant=True,\n     bnb_4bit_quant_type=\"nf4\",\n     bnb_4bit_compute_dtype=torch.bfloat16\n )\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config\n)\n</pre> from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig import torch  model_name = \"mhenrichsen/danskgpt-tiny-chat\" # download a smaller model (due to memory constraint of Colab) # model_name=\"danish-foundation-models/munin-7b-alpha\" # if you have more memory you can use this   # Load base model # - optionally load the model in 4-bit precision (recommended for large models to save memory) bnb_config = BitsAndBytesConfig(      load_in_4bit=True,      bnb_4bit_use_double_quant=True,      bnb_4bit_quant_type=\"nf4\",      bnb_4bit_compute_dtype=torch.bfloat16  ) model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config ) <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n</pre> In\u00a0[4]: Copied! <pre>from transformers import TextStreamer, AutoTokenizer\n\nprompt = \"Meningen med livet er\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer([prompt], return_tensors=\"pt\")\nstreamer = TextStreamer(tokenizer)\noutputs = model.generate(**inputs, streamer=streamer, max_new_tokens=50)\n# The output is influence by quantization (if the model is not trained with quantization)\n# Try disabling it to see the difference.\n</pre> from transformers import TextStreamer, AutoTokenizer  prompt = \"Meningen med livet er\"  tokenizer = AutoTokenizer.from_pretrained(model_name) inputs = tokenizer([prompt], return_tensors=\"pt\") streamer = TextStreamer(tokenizer) outputs = model.generate(**inputs, streamer=streamer, max_new_tokens=50) # The output is influence by quantization (if the model is not trained with quantization) # Try disabling it to see the difference. <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>&lt;s&gt; Meningen med livet </pre> <pre>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n</pre> <pre>er at finde en balance mellem arbejde og fritid. Det er ikke n\u00f8dvendigt at have en stor m\u00e6ngde penge for at have det godt. Det er vigtigt at have\n</pre> In\u00a0[5]: Copied! <pre>from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare quantized model for peft training\nmodel = prepare_model_for_kbit_training(model)\n\n# create lora confgi\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Create PeftModel which inserts LoRA adaper modules into the model\nmodel = get_peft_model(model, lora_config)\n\n# to save the adapter weights (not the model weights)\n# model.save_pretrained(\"my_awesome_adapter\")\n</pre> from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training  # Prepare quantized model for peft training model = prepare_model_for_kbit_training(model)  # create lora confgi lora_config = LoraConfig(     r=8,     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],     bias=\"none\",     task_type=TaskType.CAUSAL_LM, )  # Create PeftModel which inserts LoRA adaper modules into the model model = get_peft_model(model, lora_config)  # to save the adapter weights (not the model weights) # model.save_pretrained(\"my_awesome_adapter\") <p>Load the dataset from Hugging Face Hub or use local data. In this example, we will use the <code>kobprof/skolegpt-instruct</code> dataset from the Hugging Face Hub. The dataset is a Danish instruction dataset that has been translated from English to Danish.</p> <p>Examining the data you can see that it would be easy to replace it with your own dataset.</p> In\u00a0[6]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</pre> from datasets import load_dataset  dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\") print(f\"Number of samples in dataset: {len(dataset):,}\") <pre>Number of samples in dataset: 21,580\n</pre> In\u00a0[7]: Copied! <pre># let us examine one sample\nsample = dataset[101]\nprint(sample)\n</pre> # let us examine one sample sample = dataset[101] print(sample) <pre>{'id': 't0.1101311', 'system_prompt': 'Du er en AI-assistent. Brugeren vil give dig en opgave. Dit m\u00e5l er at udf\u00f8re opgaven s\u00e5 trofast, som du kan. Mens du udf\u00f8rer opgaven, skal du t\u00e6nke trin for trin og begrunde dine trin.', 'question': 'Hvilket amerikansk rockband fra Frederick, Maryland, med oprindelse i Germantown, Maryland, havde en live-DVD med titlen Full Fathom Five: Video Field Recordings?', 'response': 'For at finde svaret p\u00e5 dette sp\u00f8rgsm\u00e5l vil jeg f\u00f8rst huske popul\u00e6re amerikanske rockbands og tjekke, om nogen af dem er fra Frederick, Maryland, eller har forbindelser til Germantown, Maryland. Hvis jeg ikke kan finde svaret gennem min vidensbase, vil jeg s\u00f8ge p\u00e5 internettet efter relevante oplysninger.\\n\\nDa jeg genkalder mig popul\u00e6re amerikanske rockbands, kan jeg ikke umiddelbart identificere nogen bands fra Frederick eller Germantown, Maryland.\\n\\nJeg vil nu s\u00f8ge p\u00e5 internettet efter oplysningerne.\\n\\n[S\u00f8ger...]\\n\\nEfter at have s\u00f8gt p\u00e5 internettet fandt jeg det amerikanske rockband Clutch, som er fra Frederick, Maryland, og stammer fra Germantown, Maryland. De har en live-DVD, der hedder Full Fathom Five: Video Field Recordings.\\n\\nA: Det amerikanske rockband, du leder efter, er Clutch.', 'source': 't0'}\n</pre> <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> In\u00a0[8]: Copied! <pre>n_samples = 1000\nRANDOM_SEED = 42\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</pre> n_samples = 1000 RANDOM_SEED = 42 dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples)) <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> In\u00a0[9]: Copied! <pre>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</pre> def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:     \"\"\"This converts the sample to the standardised ChatML format.      Args:         sample:             The data sample.      Returns:         The sample set up in the ChatML format.     \"\"\"     return {         \"messages\": [             {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},             {\"role\": \"user\", \"content\": sample[\"question\"]},             {\"role\": \"assistant\", \"content\": sample[\"response\"]}         ]     }  dataset = dataset.map(create_conversation, batched=False) In\u00a0[13]: Copied! <pre>from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Setting up the Trainer\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=1024, # The maximum sequence length the model can handle\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</pre> from trl import SFTTrainer from transformers import TrainingArguments  # Setting up the Trainer FINETUNING_CONFIGURATION = dict(     per_device_train_batch_size=8,     gradient_accumulation_steps=1,     warmup_steps=5,     num_train_epochs=1,     learning_rate=2e-4,     weight_decay=0.01,     lr_scheduler_type=\"linear\", )  trainer = SFTTrainer(     model=model,     tokenizer=tokenizer,     train_dataset=dataset,     max_seq_length=1024, # The maximum sequence length the model can handle     packing=True,  # Can make training 5x faster for short sequences.     args = TrainingArguments(         optim=\"adamw_8bit\",         fp16=not torch.cuda.is_bf16_supported(),         bf16=torch.cuda.is_bf16_supported(),         logging_steps=3,         seed=RANDOM_SEED,         output_dir=\"outputs\",         **FINETUNING_CONFIGURATION     ), ) In\u00a0[14]: Copied! <pre># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</pre> # Log some GPU stats before we start the finetuning gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(     f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"     f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\" ) <pre>You're using the Tesla T4 GPU, which has 14.75 GB of memory in total, of which 6.81GB has been reserved already.\n</pre> In\u00a0[15]: Copied! <pre># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</pre> # This is where the actual finetuning is happening trainer_stats = trainer.train() <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n</pre>        [76/76 07:59, Epoch 1/1]      Step Training Loss 3 1.759400 6 1.845600 9 1.809500 12 1.581200 15 1.613000 18 1.587200 21 1.521900 24 1.490400 27 1.484400 30 1.517800 33 1.434900 36 1.434500 39 1.479500 42 1.619800 45 1.453000 48 1.324800 51 1.379600 54 1.454800 57 1.385700 60 1.366800 63 1.279800 66 1.319700 69 1.400300 72 1.390300 75 1.426700 <p> </p> In\u00a0[16]: Copied! <pre># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</pre> # Log some post-training GPU statistics used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory / max_memory * 100, 3) lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) print(     f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"     f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"     \"was used for LoRa.\" ) <pre>We ended up using 9.05 GB GPU memory (61.38%), of which 2.25 GB (15.23%) was used for LoRa.\n</pre> In\u00a0[18]: Copied! <pre>from transformers import GenerationConfig\n\nGENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256,\n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True,\n    temperature=0.2,\n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,\n)\n</pre> from transformers import GenerationConfig  GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,      # Controlling how the model chooses the next token to generate     do_sample=True,     temperature=0.2,     repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id,     use_cache=False, ) <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> In\u00a0[20]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"N\u00e6vn nogle positive og negative sider ved large language models.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"N\u00e6vn nogle positive og negative sider ved large language models.\"  # And change this too     ), ]  outputs = model.generate(     input_ids=tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),     streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <pre>Large Language Models (LLM) er en type af maskinl\u00e6ringsteknologier, der bruges til at generere eller forudsige tekster p\u00e5 et stort antal sprog. De har flere fordele:\n\n1. Genereret tekst: LLM-modeller kan generere tekst baseret p\u00e5 inputdata fra store m\u00e6ngder af data. Dette g\u00f8r det muligt at generere meget omfattende og detaljerede tekster med h\u00f8j grad af n\u00f8jagtighed.\n\n2. Forbedret tekstbehandling: LLM-modeller kan behandle store m\u00e6ngder af tekst i realtid, hvilket betyder, at de ikke skal vente p\u00e5, at inputdata bliver indsamlet f\u00f8rst. Dette reducerer tiden, det tager at generere en tekst, og giver dem mulighed for at fokusere mere p\u00e5 den specifikke opgave.\n\n3. Brugervenlig: LLM-modeller er designet til at v\u00e6re nemme at bruge for mennesker,\n</pre> <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)"},{"location":"tutorials/finetune_qlora/#draft-false-date-2024-05-08","title":"draft: false date: 2024-05-08\u00b6","text":""},{"location":"tutorials/finetune_qlora/#finetuning-language-models-using-qlora","title":"Finetuning Language Models using QLoRA\u00b6","text":""},{"location":"tutorials/finetune_qlora/#open-in-colab","title":"Open In Colab\u00b6","text":"<p>You can open this notebook in Google Colab by clicking the button below:</p> <p></p>"},{"location":"tutorials/finetune_qlora/#introduction","title":"Introduction\u00b6","text":"<p>Large Language Models (LLMs) have shown impressive capabilities in a wide variety of applications. Developers often seek to tailor these LLMs for specific use-cases and applications to fine-tune them for better performance or other reasons including but not limited to:</p> <ul> <li>Reducing Hallucinations</li> <li>Better handling of retrieved information</li> <li>Learn New Information (When data size is large)</li> <li>Cost Optimization</li> <li>Privacy</li> </ul> <p> </p> Figure: An simple illustration of model fine-tuning. <p>However, LLMs are large by design and require a large number of GPUs to be fine-tuned. A common approach to fine-tuning LLMs is to use a technique called Parameter Efficient Fine-Tuning (PEFT). PEFT methods aim to drastically reduce the number of trainable parameters of a model while keeping the same performance as full fine-tuning. The following sections will introduce the LoRA method, but it is perfectly fine to skip this section.</p> An example of the memory requirements for fine-tuning a large language model (click to unfold)  <p>Let\u2019s focus on a specific example by trying to fine-tune a Llama model on a free-tier Google Colab instance (1x NVIDIA T4 16GB). Llama-2 7B has 7 billion parameters, with a total of 28GB in case the model is loaded in full-precision. Given our GPU memory constraint (16GB), the model cannot even be loaded, much less trained on our GPU. This memory requirement can be divided by two with negligible performance degradation. You can read more about running models in half-precision and mixed precision for training here.</p> <p>In the case of full fine-tuning with Adam optimizer using a half-precision model and mixed-precision mode, we need to allocate per parameter:</p> <ul> <li>2 bytes for the weight</li> <li>2 bytes for the gradient</li> <li>4 + 8 bytes for the Adam optimizer states</li> </ul> <p>With a total of 16 bytes per trainable parameter, this makes a total of 112GB (excluding the intermediate hidden states). Given that the largest GPU available today can have up to 80GB GPU VRAM, it makes fine-tuning challenging and less accessible to everyone. To bridge this gap, Parameter Efficient Fine-Tuning (PEFT) methods are largely adopted today by the community.</p>"},{"location":"tutorials/finetune_qlora/#low-rank-adaption-for-large-language-models-lora-parameter-efficient-fine-tuning","title":"Low-rank Adaption for Large Language Models (LoRA) Parameter Efficient Fine-Tuning\u00b6","text":"<p>Parameter Efficient Fine-Tuning (PEFT) methods, such as LoRA, aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning. Multiple PEFT methods to get an overview we recommend the article \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\", however in this notebook we will focus on the LoRA method.</p> <p>The LoRA method by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model(that we will denote by base model).</p> <p>To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn\u2019t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.</p> <p>This approach has several advantages:</p> <ul> <li>LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.</li> <li>The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.</li> <li>LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.</li> <li>The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.</li> <li>LoRA does not add any inference latency when adapter weights are merged with the base model</li> </ul> <p>In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix.</p> <p> </p> Figure: Animated diagram that show how LoRA works in practice."},{"location":"tutorials/finetune_qlora/#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>Before we start, we need to install the following dependencies:</p>"},{"location":"tutorials/finetune_qlora/#loading-and-testing-model","title":"Loading and testing Model\u00b6","text":"<p>This sections loads the model and tests it on a simple example. For this example, we will use the <code>munin-7b-alpha</code> model created by the Danish Foundation Models team.</p>"},{"location":"tutorials/finetune_qlora/#add-in-the-lora-adapters","title":"Add in the LoRA Adapters\u00b6","text":"<p>This section adds in the LoRA adapters to the model. The LoRA adapters are added to the attention blocks of the model. The adapters are initialized with random values and are trained during the fine-tuning process. The original weights of the model are kept frozen and are not updated during the fine-tuning process. The adapters are merged with the original weights during inference to produce the final results.</p>"},{"location":"tutorials/finetune_qlora/#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":""},{"location":"tutorials/finetune_qlora/#finetuning-the-model","title":"Finetuning the Model\u00b6","text":"<p>We will use the <code>trl</code> library to finetune the model. <code>trl</code> is a library which provides a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. In this notebook, we will only use the SFT step.</p>"},{"location":"tutorials/finetune_qlora/#trying-out-the-new-model","title":"Trying out the new Model\u00b6","text":"<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p>"},{"location":"tutorials/finetune_qlora/#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/finetune_qlora/#references","title":"References\u00b6","text":"<p>This notebook takes inspiration, snippets, figures, and quotes from the following sources:</p> <ul> <li>Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem</li> <li>Our previous tutorial on (efficiently) finetuning language models</li> <li>Enhancing LLM inferencing with RAG and fine-tuned LLMs - Generative AI Workshop, AI-ML Systems Conference - 2023, Bengaluru</li> </ul>"},{"location":"tutorials/merge/","title":"Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook. It has descriptions and examples for the different merge methods it supports.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</pre> # Uncomment to install packages (already done for you) # !git clone https://github.com/cg123/mergekit.git # %cd mergekit # %pip install -e . # %cd .. In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import shutil from pathlib import Path  # For merging the models from mergekit.config import MergeConfiguration from mergekit.merge import MergeOptions, run_merge  # For loading the models and running them after the merge import transformers from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> import getpass HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None <p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> In\u00a0[\u00a0]: Copied! <pre>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</pre> merge_config = dict(     models=[         dict(             model=\"danish-foundation-models/munin-7b-alpha\",         ),         dict(             model=\"mlabonne/NeuralBeagle14-7B\",             parameters=dict(                 density=0.53,                 weight=0.6,             ),         ),     ],     merge_method=\"dare_ties\",     base_model=\"danish-foundation-models/munin-7b-alpha\",     parameters=dict(         int8_mask=True,     ),     dtype=\"bfloat16\", ) In\u00a0[\u00a0]: Copied! <pre>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</pre> LAZY_UNPICKLE = False  # Experimental low-memory model loader LOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap OUT_PATH = \"./merged\" In\u00a0[\u00a0]: Copied! <pre>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</pre> run_merge(     MergeConfiguration.model_validate(merge_config),     out_path=OUT_PATH,     options=MergeOptions(         lora_merge_cache=\"/tmp\",         cuda=torch.cuda.is_available(),         copy_tokenizer=True,         lazy_unpickle=LAZY_UNPICKLE,         low_cpu_memory=LOW_CPU_MEMORY,     ) ) <p>Time to try out the new merged model. Let's start by loading it from disk.</p> In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</pre> model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True) tokenizer = AutoTokenizer.from_pretrained(OUT_PATH)  # Choosing a chat template for a merged model can be difficult. The one defined in  # NeuralBeagle seems broken. Additionally, it does not have special tokens that some  # of the merged models might have been trained with tokenizer.chat_template = \"\"\" {% if not add_generation_prompt is defined %}     {% set add_generation_prompt = false %} {% endif %} {% for message in messages %}     {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}} {% endfor %} {% if add_generation_prompt %}     {{ '&lt;|im_start|&gt;assistant\\n' }} {% endif %} \"\"\"  pipeline = transformers.pipeline(     \"text-generation\",     model=model,     tokenizer=tokenizer,     device_map=\"auto\", ) <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id ) In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad er en stor sprogmodel?\"  # And change this too     ), ]  outputs = pipeline(     tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),      streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN) <p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> In\u00a0[\u00a0]: Copied! <pre># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</pre> # shutil.rmtree(OUT_PATH, ignore_errors=True) # shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)"},{"location":"tutorials/merge/#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/merge/#merging-language-models","title":"Merging Language Models\u00b6","text":""},{"location":"tutorials/merge/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/merge/#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/merge/#configure-the-merge","title":"Configure the Merge\u00b6","text":""},{"location":"tutorials/merge/#merge","title":"Merge!\u00b6","text":""},{"location":"tutorials/merge/#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/merge/#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/merge/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"blog/archive/2024/","title":"2024","text":""}]}