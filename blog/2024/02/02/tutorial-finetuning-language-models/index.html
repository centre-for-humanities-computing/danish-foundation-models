
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../tutorial-merging-language-models/">
      
      
        <link rel="next" href="../../../07/04/datakilder/">
      
      
      <link rel="icon" href="../../../../../_static/icon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.3">
    
    
      
        <title>Tutorial: Finetuning Language Models - Danish Foundation Models</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tutorial-finetuning-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Danish Foundation Models" class="md-header__button md-logo" aria-label="Danish Foundation Models" data-md-component="logo">
      
  <img src="../../../../../_static/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Danish Foundation Models
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tutorial: Finetuning Language Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/centre-for-humanities-computing/danish-foundation-models" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../models/" class="md-tabs__link">
        
  
    
  
  Models

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../dcc/" class="md-tabs__link">
          
  
    
  
  Datasets

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Blog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Danish Foundation Models" class="md-nav__button md-logo" aria-label="Danish Foundation Models" data-md-component="logo">
      
  <img src="../../../../../_static/icon.png" alt="logo">

    </a>
    Danish Foundation Models
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/centre-for-humanities-computing/danish-foundation-models" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Datasets
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Datasets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../dcc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Datasheets
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Datasheets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../datasheets/danews/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DaNews
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../datasheets/hopetwitter/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HopeTwitter
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../datasheets/netarkivet_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NAT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../datasheets/daradio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DaRadio
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5v-5Z"/></svg>
                        <time datetime="2024-02-02 00:00:00" class="md-ellipsis">February 2, 2024</time>
                      </div>
                    </li>
                    
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7h1.5Z"/></svg>
                          <span class="md-ellipsis">
                            
                              5 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
          </nav>
          
            

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#install-dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Install Dependencies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#get-hugging-face-token" class="md-nav__link">
    <span class="md-ellipsis">
      Get Hugging Face Token
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configure-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      Configure the Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      Load the Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#load-and-prepare-data" class="md-nav__link">
    <span class="md-ellipsis">
      Load and Prepare Data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#finetune" class="md-nav__link">
    <span class="md-ellipsis">
      Finetune!
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#try-it-out" class="md-nav__link">
    <span class="md-ellipsis">
      Try it Out
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#share-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      Share the Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#extra-export-model-to-other-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Extra: Export Model to Other Frameworks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Extra: Export Model to Other Frameworks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#saving-to-float16-for-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Saving to float16 for vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gguf-llamacpp-conversion" class="md-nav__link">
    <span class="md-ellipsis">
      GGUF / llama.cpp Conversion
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        

  
    <a href="https://github.com/centre-for-humanities-computing/danish-foundation-models/blob/main/docs/blog/posts/finetune.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


<h1 id="tutorial-finetuning-language-models">Tutorial: Finetuning Language Models<a class="headerlink" href="#tutorial-finetuning-language-models" title="Permanent link">&para;</a></h1>
<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p>
<p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p>
<!-- more -->

<h2 id="install-dependencies">Install Dependencies<a class="headerlink" href="#install-dependencies" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="c1"># Uncomment to install packages (already done for you)</span>
<span class="c1"># %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121</span>
<span class="c1"># %pip install &quot;unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># General packages</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">getpass</span>

<span class="c1"># For loading the finetuning datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># For loading and finetuning the models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span><span class="p">,</span> <span class="n">setup_chat_format</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span><span class="p">,</span> <span class="n">GenerationConfig</span>
</code></pre></div>
<h2 id="get-hugging-face-token">Get Hugging Face Token<a class="headerlink" href="#get-hugging-face-token" title="Permanent link">&para;</a></h2>
<p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p>
<p>You can generate a token at https://hf.co/settings/tokens.</p>
<p>If you don't want to supply a token then simply leave it blank!</p>
<div class="highlight"><pre><span></span><code><span class="n">HUGGING_FACE_TOKEN</span> <span class="o">=</span> <span class="n">getpass</span><span class="o">.</span><span class="n">getpass</span><span class="p">(</span><span class="s2">&quot;Hugging Face Token: &quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">HUGGING_FACE_TOKEN</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Not using a Hugging Face token.&quot;</span><span class="p">)</span>
    <span class="n">HUGGING_FACE_TOKEN</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>
<h2 id="configure-the-model">Configure the Model<a class="headerlink" href="#configure-the-model" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">MODEL_CONFIGURATION</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;danish-foundation-models/munin-7b-alpha&quot;</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>  
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.</span>
    <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;flash_attention_2&quot;</span>
<span class="p">)</span>

<span class="n">PEFT_CONFIGURATION</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">r</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;down_proj&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">lora_alpha</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># Supports any, but = 0 is optimized</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>  <span class="c1"># Supports any, but = &quot;none&quot; is optimized</span>
    <span class="n">use_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_rslora</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Support rank stabilized LoRA</span>
    <span class="n">loftq_config</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># And LoftQ</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">RANDOM_SEED</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">FINETUNING_CONFIGURATION</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<h2 id="load-the-model">Load the Model<a class="headerlink" href="#load-the-model" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="o">**</span><span class="n">MODEL_CONFIGURATION</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">HUGGING_FACE_TOKEN</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">setup_chat_format</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">PEFT_CONFIGURATION</span><span class="p">)</span>
</code></pre></div>
<h2 id="load-and-prepare-data">Load and Prepare Data<a class="headerlink" href="#load-and-prepare-data" title="Permanent link">&para;</a></h2>
<p>Load the dataset from Hugging Face Hub:</p>
<div class="highlight"><pre><span></span><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;kobprof/skolegpt-instruct&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of samples in dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p>
<div class="highlight"><pre><span></span><code><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">RANDOM_SEED</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
</code></pre></div>
<p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">create_conversation</span><span class="p">(</span><span class="n">sample</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This converts the sample to the standardised ChatML format.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample:</span>
<span class="sd">            The data sample.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The sample set up in the ChatML format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;system_prompt&quot;</span><span class="p">]},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;response&quot;</span><span class="p">]}</span>
        <span class="p">]</span>
    <span class="p">}</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">create_conversation</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<h2 id="finetune">Finetune!<a class="headerlink" href="#finetune" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MODEL_CONFIGURATION</span><span class="p">[</span><span class="s2">&quot;max_seq_length&quot;</span><span class="p">],</span>
    <span class="n">dataset_num_proc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Can make training 5x faster for short sequences.</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;adamw_8bit&quot;</span><span class="p">,</span>
        <span class="n">fp16</span><span class="o">=</span><span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">(),</span>
        <span class="n">bf16</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">(),</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">RANDOM_SEED</span><span class="p">,</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;outputs&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">FINETUNING_CONFIGURATION</span>
    <span class="p">),</span>
<span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Log some GPU stats before we start the finetuning</span>
<span class="n">gpu_stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">start_gpu_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">max_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">gpu_stats</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;You&#39;re using the </span><span class="si">{</span><span class="n">gpu_stats</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> GPU, which has </span><span class="si">{</span><span class="n">max_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB of memory &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;in total, of which </span><span class="si">{</span><span class="n">start_gpu_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB has been reserved already.&quot;</span>
<span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># This is where the actual finetuning is happening</span>
<span class="n">trainer_stats</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Log some post-training GPU statistics</span>
<span class="n">used_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">used_memory_for_lora</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory</span> <span class="o">-</span> <span class="n">start_gpu_memory</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">used_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory</span> <span class="o">/</span> <span class="n">max_memory</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">lora_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory_for_lora</span> <span class="o">/</span> <span class="n">max_memory</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;We ended up using </span><span class="si">{</span><span class="n">used_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB GPU memory (</span><span class="si">{</span><span class="n">used_percentage</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%), &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;of which </span><span class="si">{</span><span class="n">used_memory_for_lora</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB (</span><span class="si">{</span><span class="n">lora_percentage</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%) &quot;</span>
    <span class="s2">&quot;was used for LoRa.&quot;</span>
<span class="p">)</span>
</code></pre></div>
<h2 id="try-it-out">Try it Out<a class="headerlink" href="#try-it-out" title="Permanent link">&para;</a></h2>
<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p>
<p>You can leave the following config as-is, or you can experiment. <a href="https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig">Here</a> is a list of all the different arguments.</p>
<div class="highlight"><pre><span></span><code><span class="n">GENERATION_CONFIG</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
    <span class="c1"># What should be outputted</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> 

    <span class="c1"># Controlling how the model chooses the next token to generate</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> 
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>

    <span class="c1"># Miscellaneous required settings</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Required by unsloth</span>
<span class="p">)</span>
</code></pre></div>
<p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p>
<div class="highlight"><pre><span></span><code><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nb">dict</span><span class="p">(</span>
        <span class="n">role</span><span class="o">=</span><span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;&quot;</span>  <span class="c1"># Change this to anything you want</span>
    <span class="p">),</span>
    <span class="nb">dict</span><span class="p">(</span>
        <span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.&quot;</span>  <span class="c1"># And change this too</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span>
    <span class="n">streamer</span><span class="o">=</span><span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">generation_config</span><span class="o">=</span><span class="n">GENERATION_CONFIG</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<h2 id="share-the-model">Share the Model<a class="headerlink" href="#share-the-model" title="Permanent link">&para;</a></h2>
<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># model.push_to_hub(&quot;your_name/qlora_model&quot;, token=HUGGING_FACE_TOKEN)</span>
</code></pre></div>
<h2 id="extra-export-model-to-other-frameworks">Extra: Export Model to Other Frameworks<a class="headerlink" href="#extra-export-model-to-other-frameworks" title="Permanent link">&para;</a></h2>
<h3 id="saving-to-float16-for-vllm">Saving to float16 for vLLM<a class="headerlink" href="#saving-to-float16-for-vllm" title="Permanent link">&para;</a></h3>
<p>The popular inference framework <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a> can take advantage of having a model available in lower precision, enabling faster inference times.</p>
<p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Merge to 16bit</span>
<span class="c1"># model.save_pretrained_merged(&quot;model&quot;, tokenizer, save_method=&quot;merged_16bit&quot;,)</span>
<span class="c1"># model.push_to_hub_merged(&quot;hf/model&quot;, tokenizer, save_method=&quot;merged_16bit&quot;, token=HUGGING_FACE_TOKEN)</span>

<span class="c1"># Merge to 4bit</span>
<span class="c1"># model.save_pretrained_merged(&quot;model&quot;, tokenizer, save_method=&quot;merged_4bit&quot;,)</span>
<span class="c1"># model.push_to_hub_merged(&quot;hf/model&quot;, tokenizer, save_method=&quot;merged_4bit&quot;, token=HUGGING_FACE_TOKEN)</span>
</code></pre></div>
<p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Just LoRA adapters</span>
<span class="c1"># model.save_pretrained_merged(&quot;model&quot;, tokenizer, save_method=&quot;lora&quot;,)</span>
<span class="c1"># model.push_to_hub_merged(&quot;hf/model&quot;, tokenizer, save_method=&quot;lora&quot;, token=HUGGING_FACE_TOKEN)</span>
</code></pre></div>
<h3 id="gguf-llamacpp-conversion">GGUF / llama.cpp Conversion<a class="headerlink" href="#gguf-llamacpp-conversion" title="Permanent link">&para;</a></h3>
<p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Save to 8bit Q8_0</span>
<span class="c1"># model.save_pretrained_gguf(&quot;model&quot;, tokenizer)</span>
<span class="c1"># model.push_to_hub_gguf(&quot;hf/model&quot;, tokenizer, token=HUGGING_FACE_TOKEN)</span>

<span class="c1"># Save to 16bit GGUF</span>
<span class="c1"># model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method=&quot;f16&quot;)</span>
<span class="c1"># model.push_to_hub_gguf(&quot;hf/model&quot;, tokenizer, quantization_method=&quot;f16&quot;, token=HUGGING_FACE_TOKEN)</span>

<span class="c1"># Save to q4_k_m GGUF</span>
<span class="c1"># model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method=&quot;q4_k_m&quot;)</span>
<span class="c1"># model.push_to_hub_gguf(&quot;hf/model&quot;, tokenizer, quantization_method=&quot;q4_k_m&quot;, token=HUGGING_FACE_TOKEN)</span>
</code></pre></div>
<p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going <a href="https://gpt4all.io/index.html">here</a>.</p>







  
  




  



      
    </article>
  </div>

          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
        
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../tutorial-merging-language-models/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Tutorial: Merging Language Models">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Tutorial: Merging Language Models
              </div>
            </div>
          </a>
        
        
          
          <a href="../../../07/04/datakilder/" class="md-footer__link md-footer__link--next" aria-label="Next: Datakilder">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Datakilder
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Danish Foundation Models Project
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/centre-for-humanities-computing/danish-foundation-models" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.tooltips", "navigation.footer", "navigation.indexes", "toc.follow", "pymdownx.caret", "pymdownx.tilde", "content.action.edit"], "search": "../../../../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
    
  </body>
</html>