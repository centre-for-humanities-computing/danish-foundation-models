{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouy1LbXc_dY5"
      },
      "source": [
        "---\n",
        "draft: false\n",
        "date: 2024-05-08\n",
        "---\n",
        "\n",
        "\n",
        "# Finetuning Language Models using QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3xAqrHd_dY8"
      },
      "source": [
        "We demonstrate how to finetune a [`munin-7b-alpha`](https://huggingface.co/danish-foundation-models/munin-7b-alpha) or another large language model (LLM) on a Danish translated instruction tuning dataset, with LoRA and tools from the PyTorch and Hugging Face ecosystem. This notebook can be run on on a typical consumer GPU (e.g. NVIDIA T4 16GB).\n",
        "\n",
        "<!-- more -->\n",
        "This notebook takes some liberties to ensure simplicity and readability, while remaining reasonably efficient. However if you want a more efficient approach, see the [tutorial on (efficiently) finetuning language models](https://www.foundationmodels.dk/blog/2024/02/02/tutorial-finetuning-language-models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgrJHl6W_dY9"
      },
      "source": [
        "### Open In Colab\n",
        "\n",
        "You can open this notebook in Google Colab by clicking the button below:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/centre-for-humanities-computing/danish-foundation-models/blob/main/docs/tutorials/finetune.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz6KepHd_dY9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Large Language Models (LLMs) have shown impressive capabilities in a wide variety of applications. Developers often seek to tailor these LLMs for specific use-cases and applications to fine-tune them for better performance or other reasons including but not limited to:\n",
        "\n",
        "- Reducing Hallucinations\n",
        "- Better handling of retrieved information\n",
        "- Learn New Information (When data size is large)\n",
        "- Cost Optimization\n",
        "- Privacy\n",
        "\n",
        "<figure>\n",
        "<p align=\"center\">\n",
        "    <img src=\"finetune.png\" alt=\"finetune\" style=\"width: 800px;\"/>\n",
        "</p>\n",
        "    <figcaption>Figure: An simple illustration of model fine-tuning.</figcaption>\n",
        "</figure>\n",
        "\n",
        "However, LLMs are large by design and require a large number of GPUs to be fine-tuned. A common approach to fine-tuning LLMs is to use a technique called Parameter Efficient Fine-Tuning (PEFT). PEFT methods aim to drastically reduce the number of trainable parameters of a model while keeping the same performance as full fine-tuning. The following sections will introduce the LoRA method, but it is perfectly fine to skip this section.\n",
        "\n",
        "<details>\n",
        "<summary>An example of the memory requirements for fine-tuning a large language model (click to unfold) </summary>\n",
        "\n",
        "Let’s focus on a specific example by trying to fine-tune a Llama model on a free-tier Google Colab instance (1x NVIDIA T4 16GB). Llama-2 7B has 7 billion parameters, with a total of 28GB in case the model is loaded in full-precision. Given our GPU memory constraint (16GB), the model cannot even be loaded, much less trained on our GPU. This memory requirement can be divided by two with negligible performance degradation. You can read more about running models in half-precision and mixed precision for training here.\n",
        "\n",
        "In the case of full fine-tuning with Adam optimizer using a half-precision model and mixed-precision mode, we need to allocate per parameter:\n",
        "\n",
        "- 2 bytes for the weight\n",
        "- 2 bytes for the gradient\n",
        "- 4 + 8 bytes for the Adam optimizer states\n",
        "\n",
        "With a total of 16 bytes per trainable parameter, this makes a total of 112GB (excluding the intermediate hidden states). Given that the largest GPU available today can have up to 80GB GPU VRAM, it makes fine-tuning challenging and less accessible to everyone. To bridge this gap, Parameter Efficient Fine-Tuning (PEFT) methods are largely adopted today by the community.\n",
        "\n",
        "</details>\n",
        "\n",
        "### Low-rank Adaption for Large Language Models (LoRA) Parameter Efficient Fine-Tuning\n",
        "\n",
        "Parameter Efficient Fine-Tuning (PEFT) methods, such as LoRA, aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning. Multiple PEFT methods to get an overview we recommend the article \"[Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647)\", however in this notebook we will focus on the LoRA method.\n",
        "\n",
        "The [LoRA method](https://arxiv.org/pdf/2106.09685) by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model(that we will denote by base model).\n",
        "\n",
        "To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n",
        "\n",
        "This approach has several advantages:\n",
        "\n",
        "- LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.\n",
        "- The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.\n",
        "- LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.\n",
        "- The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.\n",
        "- LoRA does not add any inference latency when adapter weights are merged with the base model\n",
        "\n",
        "In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix.\n",
        "\n",
        "<figure>\n",
        "<p align=\"center\">\n",
        "    <img src=\"fg2.gif\" alt=\"lora\" style=\"width: 500px;\"/>\n",
        "</p>\n",
        "    <figcaption>Figure: Animated diagram that show how LoRA works in practice.</figcaption>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DtFjdgA_dY-"
      },
      "source": [
        "\n",
        "\n",
        "## Install Dependencies\n",
        "Before we start, we need to install the following dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dT0fCx_4_dY-"
      },
      "outputs": [],
      "source": [
        "%pip install -q datasets bitsandbytes peft trl accelerate sentencepiece protobuf --upgrade\n",
        "\n",
        "# Description of the libraries:\n",
        "# - Datasets: A high-performant dataset library for easily sharing and accessing datasets from the huggingface Hub at huggingface.co/datasets\n",
        "# - bitsandbytes: A lightweight library for loading models using low-precession (this makes it faster and use less memory)\n",
        "# - Transformers: A high-level library for working with language LLMs\n",
        "# - PEFT: A library for parameter-efficient fine-tuning of LLMs\n",
        "# - TRL: A library for training LLMs using reinforcement learning\n",
        "# - Accelerate: A library for distributed and efficient training of LLMs\n",
        "# - Sentencepiece: A library for tokenizing text required by some models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the version of the libraries for reproducibility\n",
        "import datasets\n",
        "import bitsandbytes\n",
        "import transformers\n",
        "import peft\n",
        "import trl\n",
        "import accelerate\n",
        "import sentencepiece\n",
        "\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"peft: {peft.__version__}\")\n",
        "print(f\"trl: {trl.__version__}\")\n",
        "print(f\"accelerate: {accelerate.__version__}\")\n",
        "print(f\"sentencepiece: {sentencepiece.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bx-r7fBVjjh",
        "outputId": "019ed5d1-b539-467d-9c7c-a64fdfcbb2d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets: 2.19.1\n",
            "bitsandbytes: 0.43.1\n",
            "transformers: 4.40.1\n",
            "peft: 0.10.0\n",
            "trl: 0.8.6\n",
            "accelerate: 0.30.0\n",
            "sentencepiece: 0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTLVo8Lv_dZA"
      },
      "source": [
        "# Loading and testing Model\n",
        "This sections loads the model and tests it on a simple example. For this example, we will use the [`munin-7b-alpha`](https://huggingface.co/danish-foundation-models/munin-7b-alpha) model created by the Danish Foundation Models team.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cM-C86dU_dZA",
        "outputId": "51d48409-3d8d-4ff8-d128-e6d3d882320d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"mhenrichsen/danskgpt-tiny-chat\" # download a smaller model (due to memory constraint of Colab)\n",
        "# model_name=\"danish-foundation-models/munin-7b-alpha\" # if you have more memory you can use this\n",
        "\n",
        "\n",
        "# Load base model\n",
        "# - optionally load the model in 4-bit precision (recommended for large models to save memory)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "     load_in_4bit=True,\n",
        "     bnb_4bit_use_double_quant=True,\n",
        "     bnb_4bit_quant_type=\"nf4\",\n",
        "     bnb_4bit_compute_dtype=torch.bfloat16\n",
        " )\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3VTP4KL2_dZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750a3e82-3bc1-4ad3-faf9-1704c9323973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Meningen med livet "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "er at finde en balance mellem arbejde og fritid. Det er ikke nødvendigt at have en stor mængde penge for at have det godt. Det er vigtigt at have\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer, AutoTokenizer\n",
        "\n",
        "prompt = \"Meningen med livet er\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "outputs = model.generate(**inputs, streamer=streamer, max_new_tokens=50)\n",
        "# The output is influence by quantization (if the model is not trained with quantization)\n",
        "# Try disabling it to see the difference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add in the LoRA Adapters\n",
        "\n",
        "This section adds in the LoRA adapters to the model. The LoRA adapters are added to the attention blocks of the model. The adapters are initialized with random values and are trained during the fine-tuning process. The original weights of the model are kept frozen and are not updated during the fine-tuning process. The adapters are merged with the original weights during inference to produce the final results."
      ],
      "metadata": {
        "id": "QWLbMmiZXQxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare quantized model for peft training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# create lora confgi\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Create PeftModel which inserts LoRA adaper modules into the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# to save the adapter weights (not the model weights)\n",
        "# model.save_pretrained(\"my_awesome_adapter\")"
      ],
      "metadata": {
        "id": "csrl3BAeLKzQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "## Load and Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02VqE0Vz_dZB"
      },
      "source": [
        "Load the dataset from Hugging Face Hub or use local data. In this example, we will use the `kobprof/skolegpt-instruct` dataset from the Hugging Face Hub. The dataset is a Danish instruction dataset that has been translated from English to Danish.\n",
        "\n",
        "Examining the data you can see that it would be easy to replace it with your own dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ln0JarRA_dZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5457a30-71a7-4b77-c1c2-030a516ee533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in dataset: 21,580\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\n",
        "print(f\"Number of samples in dataset: {len(dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m7g6o2IF_dZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd19e1e-b3c8-4af5-85b1-127c11db5147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 't0.1101311', 'system_prompt': 'Du er en AI-assistent. Brugeren vil give dig en opgave. Dit mål er at udføre opgaven så trofast, som du kan. Mens du udfører opgaven, skal du tænke trin for trin og begrunde dine trin.', 'question': 'Hvilket amerikansk rockband fra Frederick, Maryland, med oprindelse i Germantown, Maryland, havde en live-DVD med titlen Full Fathom Five: Video Field Recordings?', 'response': 'For at finde svaret på dette spørgsmål vil jeg først huske populære amerikanske rockbands og tjekke, om nogen af dem er fra Frederick, Maryland, eller har forbindelser til Germantown, Maryland. Hvis jeg ikke kan finde svaret gennem min vidensbase, vil jeg søge på internettet efter relevante oplysninger.\\n\\nDa jeg genkalder mig populære amerikanske rockbands, kan jeg ikke umiddelbart identificere nogen bands fra Frederick eller Germantown, Maryland.\\n\\nJeg vil nu søge på internettet efter oplysningerne.\\n\\n[Søger...]\\n\\nEfter at have søgt på internettet fandt jeg det amerikanske rockband Clutch, som er fra Frederick, Maryland, og stammer fra Germantown, Maryland. De har en live-DVD, der hedder Full Fathom Five: Video Field Recordings.\\n\\nA: Det amerikanske rockband, du leder efter, er Clutch.', 'source': 't0'}\n"
          ]
        }
      ],
      "source": [
        "# let us examine one sample\n",
        "sample = dataset[101]\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWkcX3a_dZB"
      },
      "source": [
        "We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2fh9QDZf_dZB"
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\n",
        "RANDOM_SEED = 42\n",
        "dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Kkck6t_dZB"
      },
      "source": [
        "Lastly, we set up the conversations in the dataset into the standard ChatML format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "def create_conversation(sample: dict) -> dict[str, list[dict[str, str]]]:\n",
        "    \"\"\"This converts the sample to the standardised ChatML format.\n",
        "\n",
        "    Args:\n",
        "        sample:\n",
        "            The data sample.\n",
        "\n",
        "    Returns:\n",
        "        The sample set up in the ChatML format.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n",
        "            {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
        "            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(create_conversation, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "## Finetuning the Model\n",
        "\n",
        "We will use the `trl` library to finetune the model. [`trl`](https://huggingface.co/docs/trl/index) is a library which provides a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. In this notebook, we will only use the SFT step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Setting up the Trainer\n",
        "FINETUNING_CONFIGURATION = dict(\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=1024, # The maximum sequence length the model can handle\n",
        "    packing=True,  # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        optim=\"adamw_8bit\",\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=3,\n",
        "        seed=RANDOM_SEED,\n",
        "        output_dir=\"outputs\",\n",
        "        **FINETUNING_CONFIGURATION\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SXIUykWT_dZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca6e55a-ac11-4c7f-ff17-bbea27712226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're using the Tesla T4 GPU, which has 14.75 GB of memory in total, of which 6.81GB has been reserved already.\n"
          ]
        }
      ],
      "source": [
        "# Log some GPU stats before we start the finetuning\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(\n",
        "    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n",
        "    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "c23d3eba-62dc-40fd-9be4-b324990057eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [76/76 07:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.845600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.809500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.581200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.484400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.517800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.434900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.434500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.479500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.453000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.454800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.385700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.366800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.279800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.390300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.426700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# This is where the actual finetuning is happening\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qpd3VN98_dZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4694c735-bf5a-4ff8-d547-d7bf949a46f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We ended up using 9.05 GB GPU memory (61.38%), of which 2.25 GB (15.23%) was used for LoRa.\n"
          ]
        }
      ],
      "source": [
        "# Log some post-training GPU statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(\n",
        "    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n",
        "    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n",
        "    \"was used for LoRa.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvnNKt7a_dZD"
      },
      "source": [
        "## Trying out the new Model\n",
        "Time to try out the new finetuned model. First we need to set up how to generate text with it.\n",
        "\n",
        "You can leave the following config as-is, or you can experiment. [Here](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) is a list of all the different arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FrqnzoxY_dZD"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "GENERATION_CONFIG = GenerationConfig(\n",
        "    # What should be outputted\n",
        "    max_new_tokens=256,\n",
        "\n",
        "    # Controlling how the model chooses the next token to generate\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "\n",
        "    # Miscellaneous required settings\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " Let's use `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e2pEuRb1r2Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c9d38b-ec7e-4e73-9a25-1ccd6aa88807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLM) er en type af maskinlæringsteknologier, der bruges til at generere eller forudsige tekster på et stort antal sprog. De har flere fordele:\n",
            "\n",
            "1. Genereret tekst: LLM-modeller kan generere tekst baseret på inputdata fra store mængder af data. Dette gør det muligt at generere meget omfattende og detaljerede tekster med høj grad af nøjagtighed.\n",
            "\n",
            "2. Forbedret tekstbehandling: LLM-modeller kan behandle store mængder af tekst i realtid, hvilket betyder, at de ikke skal vente på, at inputdata bliver indsamlet først. Dette reducerer tiden, det tager at generere en tekst, og giver dem mulighed for at fokusere mere på den specifikke opgave.\n",
            "\n",
            "3. Brugervenlig: LLM-modeller er designet til at være nemme at bruge for mennesker,\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    dict(\n",
        "        role=\"system\",\n",
        "        content=\"\"  # Change this to anything you want\n",
        "    ),\n",
        "    dict(\n",
        "        role=\"user\",\n",
        "        content=\"Nævn nogle positive og negative sider ved large language models.\"  # And change this too\n",
        "    ),\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n",
        "    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n",
        "    generation_config=GENERATION_CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9MtGmkt_dZD"
      },
      "source": [
        "# Share the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaszbSdJ_dZE"
      },
      "source": [
        "You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JqTq7Cp_dZE"
      },
      "outputs": [],
      "source": [
        "# model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tGCSEiI_dZE"
      },
      "source": [
        "# References\n",
        "\n",
        "This notebook takes inspiration, snippets, figures, and quotes from the following sources:\n",
        "\n",
        "- [Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem](https://pytorch.org/blog/finetune-llms/)\n",
        "- [Our previous tutorial on (efficiently) finetuning language models](https://www.foundationmodels.dk/blog/2024/02/02/tutorial-finetuning-language-models/)\n",
        "- [Enhancing LLM inferencing with RAG and fine-tuned LLMs - Generative AI Workshop, AI-ML Systems Conference - 2023, Bengaluru](https://github.com/abhinav-kimothi/RAG-and-Fine-Tuning/blob/main/Notebooks/Tutorial_RAG_and_fine_tuneing_28Oct23_AIMLSystems.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKn507fq_dZE"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}