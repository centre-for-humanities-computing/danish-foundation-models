{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "draft: false\n",
    "date: 2024-02-02\n",
    "---\n",
    "\n",
    "\n",
    "# Merging Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.\n",
    "\n",
    "In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!\n",
    "\n",
    "<!-- more -->\n",
    "\n",
    "The [mergekit README](https://github.com/cg123/mergekit) is good to have open for this notebook. \n",
    "It has descriptions and examples for the different merge methods it supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSXzMj4ZJI4j"
   },
   "outputs": [],
   "source": [
    "# Uncomment to install packages (already done for you)\n",
    "# !git clone https://github.com/cg123/mergekit.git\n",
    "# %cd mergekit\n",
    "# %pip install -e .\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# For merging the models\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "# For loading the models and running them after the merge\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Hugging Face Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.\n",
    "\n",
    "You can generate a token at https://hf.co/settings/tokens.\n",
    "\n",
    "If you don't want to supply a token then simply leave it blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\n",
    "if not HUGGING_FACE_TOKEN:\n",
    "    print(\"Not using a Hugging Face token.\")\n",
    "    HUGGING_FACE_TOKEN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we set up which models we would like to merge, and which merging method to use.\n",
    "\n",
    "This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_config = dict(\n",
    "    models=[\n",
    "        dict(\n",
    "            model=\"danish-foundation-models/munin-7b-alpha\",\n",
    "        ),\n",
    "        dict(\n",
    "            model=\"mlabonne/NeuralBeagle14-7B\",\n",
    "            parameters=dict(\n",
    "                density=0.53,\n",
    "                weight=0.6,\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    merge_method=\"dare_ties\",\n",
    "    base_model=\"danish-foundation-models/munin-7b-alpha\",\n",
    "    parameters=dict(\n",
    "        int8_mask=True,\n",
    "    ),\n",
    "    dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhrYH2EeJ6h7"
   },
   "outputs": [],
   "source": [
    "LAZY_UNPICKLE = False  # Experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\n",
    "OUT_PATH = \"./merged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "YqMwZdliKBt9",
    "outputId": "0cd06a88-3440-4f06-8593-40c47c8c7eac"
   },
   "outputs": [],
   "source": [
    "run_merge(\n",
    "    MergeConfiguration.model_validate(merge_config),\n",
    "    out_path=OUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=\"/tmp\",\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=True,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to try out the new merged model. Let's start by loading it from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n",
    "\n",
    "# Choosing a chat template for a merged model can be difficult. The one defined in \n",
    "# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n",
    "# of the merged models might have been trained with\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% if not add_generation_prompt is defined %}\n",
    "    {% set add_generation_prompt = false %}\n",
    "{% endif %}\n",
    "{% for message in messages %}\n",
    "    {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "    {{ '<|im_start|>assistant\\n' }}\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. [Here](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) is a list of all the different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIG = GenerationConfig(\n",
    "    # What should be outputted\n",
    "    max_new_tokens=256, \n",
    "\n",
    "    # Controlling how the model chooses the next token to generate\n",
    "    do_sample=True, \n",
    "    temperature=0.2, \n",
    "    repetition_penalty=1.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "\n",
    "    # Miscellaneous required settings\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    dict(\n",
    "        role=\"system\",\n",
    "        content=\"\"  # Change this to anything you want\n",
    "    ),\n",
    "    dict(\n",
    "        role=\"user\",\n",
    "        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n",
    "    ),\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n",
    "    generation_config=GENERATION_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This deletes the merged model, as well as clearing the Hugging Face cache.\n",
    "\n",
    "**WARNING**: You will have to redownload any used models if you do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fvhs1oWxY3PK"
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree(OUT_PATH, ignore_errors=True)\n",
    "# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
